{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4253ae9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 2024-11-1 Python-3.9.13 torch-2.2.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training device: cpu\n",
      "Initial learning rate: 0.001\n",
      "Number of epochs: 50\n",
      "Starting training...\n",
      "Training device: cpu\n",
      "Initial learning rate: 0.0003\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1:   0%|                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_57.png, using full image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|â–Œ| 1/2 [00:06<00:06,  6.17s/it, loss=inf, char_acc=0.0490, seq_acc"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_9.png, using full image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆ| 2/2 [00:08<00:00,  4.49s/it, loss=nan, char_acc=0.0000, seq_acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_74.png, using full image\n",
      "\n",
      "Epoch 1 Results:\n",
      "Train Loss: nan, Char Acc: 0.0245, Seq Acc: 0.0000\n",
      "Val Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0000\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 2:   0%|                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_9.png, using full image\n",
      "No detection found for APS360_Project_Dataset/dataset5/img/img_57.png, using full image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆ| 2/2 [00:07<00:00,  3.99s/it, loss=nan, char_acc=0.0000, seq_acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_74.png, using full image\n",
      "\n",
      "Epoch 2 Results:\n",
      "Train Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0312\n",
      "Val Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0000\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  50%|â–Œ| 1/2 [00:05<00:05,  5.41s/it, loss=nan, char_acc=0.0000, seq_acc"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_9.png, using full image\n",
      "No detection found for APS360_Project_Dataset/dataset5/img/img_57.png, using full image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆ| 2/2 [00:08<00:00,  4.07s/it, loss=nan, char_acc=0.0000, seq_acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_74.png, using full image\n",
      "\n",
      "Epoch 3 Results:\n",
      "Train Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0312\n",
      "Val Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0000\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 4:   0%|                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_57.png, using full image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  50%|â–Œ| 1/2 [00:05<00:05,  5.45s/it, loss=nan, char_acc=0.0000, seq_acc"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_9.png, using full image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆ| 2/2 [00:08<00:00,  4.06s/it, loss=nan, char_acc=0.0000, seq_acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_74.png, using full image\n",
      "\n",
      "Epoch 4 Results:\n",
      "Train Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0000\n",
      "Val Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0000\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 5:   0%|                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_57.png, using full image\n",
      "No detection found for APS360_Project_Dataset/dataset5/img/img_9.png, using full image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆ| 2/2 [00:07<00:00,  3.95s/it, loss=nan, char_acc=0.0000, seq_acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_74.png, using full image\n",
      "\n",
      "Epoch 5 Results:\n",
      "Train Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0312\n",
      "Val Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0000\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 6:   0%|                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_9.png, using full image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:  50%|â–Œ| 1/2 [00:05<00:05,  5.46s/it, loss=nan, char_acc=0.0000, seq_acc"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_57.png, using full image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆ| 2/2 [00:08<00:00,  4.13s/it, loss=nan, char_acc=0.0000, seq_acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_74.png, using full image\n",
      "\n",
      "Epoch 6 Results:\n",
      "Train Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0000\n",
      "Val Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0000\n",
      "Learning Rate: 0.000150\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 7:   0%|                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_57.png, using full image\n",
      "No detection found for APS360_Project_Dataset/dataset5/img/img_9.png, using full image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆ| 2/2 [00:08<00:00,  4.16s/it, loss=nan, char_acc=0.0000, seq_acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_74.png, using full image\n",
      "\n",
      "Epoch 7 Results:\n",
      "Train Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0000\n",
      "Val Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0000\n",
      "Learning Rate: 0.000150\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 8:   0%|                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_57.png, using full image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8:  50%|â–Œ| 1/2 [00:05<00:05,  5.61s/it, loss=nan, char_acc=0.0000, seq_acc"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_9.png, using full image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆ| 2/2 [00:08<00:00,  4.27s/it, loss=nan, char_acc=0.0000, seq_acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_74.png, using full image\n",
      "\n",
      "Epoch 8 Results:\n",
      "Train Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0000\n",
      "Val Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0000\n",
      "Learning Rate: 0.000150\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 9:   0%|                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_9.png, using full image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:  50%|â–Œ| 1/2 [00:05<00:05,  5.70s/it, loss=nan, char_acc=0.0000, seq_acc"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_57.png, using full image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆ| 2/2 [00:08<00:00,  4.44s/it, loss=nan, char_acc=0.0000, seq_acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_74.png, using full image\n",
      "\n",
      "Epoch 9 Results:\n",
      "Train Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0312\n",
      "Val Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0000\n",
      "Learning Rate: 0.000150\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 10:   0%|                                           | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_9.png, using full image\n",
      "No detection found for APS360_Project_Dataset/dataset5/img/img_57.png, using full image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆ| 2/2 [00:14<00:00,  7.12s/it, loss=nan, char_acc=0.0000, seq_ac\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detection found for APS360_Project_Dataset/dataset5/img/img_74.png, using full image\n",
      "\n",
      "Epoch 10 Results:\n",
      "Train Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0312\n",
      "Val Loss: nan, Char Acc: 0.0000, Seq Acc: 0.0000\n",
      "Learning Rate: 0.000150\n",
      "\n",
      "Early stopping triggered!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HybridOCR' object has no attribute 'get_ensemble_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/99/qs07m3j50t3_p9zkz4_26jb40000gn/T/ipykernel_48966/3553982916.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[0;34m'scheduler_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;34m'history'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m     \u001b[0;34m'final_ensemble_weights'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhybrid_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ensemble_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m }, 'final_hybrid_model.pth')\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HybridOCR' object has no attribute 'get_ensemble_weights'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "image_dir = 'APS360_Project_Dataset/dataset5/img'\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "yolo_model = torch.hub.load('yolov5', 'custom', path='yolov5/runs/train/card_number_detector6/weights/best.pt', source = 'local').to(device)\n",
    "\n",
    "\n",
    "class DigitCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)  # 10 classes for digits 0-9\n",
    "\n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(0.1)  # After first FC layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 128 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "digit_cnn = DigitCNN()\n",
    "digit_cnn.load_state_dict(torch.load('model_checkpoints/model_epoch_30.pth', map_location=device))\n",
    "digit_cnn.to(device)\n",
    "\n",
    "class HybridOCR(nn.Module):\n",
    "    def __init__(self, num_classes=10, rnn_hidden_size=256):\n",
    "        super(HybridOCR, self).__init__()\n",
    "        \n",
    "        # CNN backbone with smaller initial channels and gradual increase\n",
    "        self.cnn = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 32 x 64\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 16 x 32\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Fourth conv block without pooling to maintain width\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Dropout for regularization\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        \n",
    "        # Calculate the feature dimensions\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        \n",
    "        # Bidirectional GRU (might work better than LSTM for this case)\n",
    "        self.rnn = nn.GRU(input_size=256 * 16,  # height after CNN\n",
    "                         hidden_size=rnn_hidden_size,\n",
    "                         num_layers=2,\n",
    "                         bidirectional=True,\n",
    "                         dropout=0.2,\n",
    "                         batch_first=True)\n",
    "        \n",
    "        # Final classification layer with proper initialization\n",
    "        self.fc = nn.Linear(rnn_hidden_size * 2, num_classes + 1)  # +1 for CTC blank\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights for better training\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, channels=1, height=32, width=128)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        conv_features = self.cnn(x)\n",
    "        \n",
    "        # Prepare for RNN\n",
    "        # Change shape from (batch, channels, height, width) to (batch, width, channels * height)\n",
    "        conv_features = conv_features.permute(0, 3, 1, 2)\n",
    "        conv_features = conv_features.contiguous().view(batch_size, -1, 256 * 16)\n",
    "        \n",
    "        # RNN\n",
    "        rnn_output, _ = self.rnn(conv_features)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fc(rnn_output)\n",
    "        \n",
    "        # Apply log softmax over character predictions\n",
    "        return F.log_softmax(output, dim=2)\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_char_acc = 0\n",
    "    total_seq_acc = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(pbar):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        batch_size = outputs.size(0)\n",
    "        \n",
    "        # Prepare CTC inputs\n",
    "        input_lengths = torch.full(size=(batch_size,),\n",
    "                                 fill_value=outputs.size(1),\n",
    "                                 dtype=torch.long,\n",
    "                                 device=device)\n",
    "        \n",
    "        target_lengths = []\n",
    "        target_labels = []\n",
    "        \n",
    "        for label in labels:\n",
    "            valid_label = label[label != 0]\n",
    "            target_lengths.append(len(valid_label))\n",
    "            target_labels.extend(valid_label.tolist())\n",
    "        \n",
    "        target_lengths = torch.tensor(target_lengths, dtype=torch.long, device=device)\n",
    "        target_labels = torch.tensor(target_labels, dtype=torch.long, device=device)\n",
    "        \n",
    "        # CTC loss calculation\n",
    "        outputs_for_loss = outputs.permute(1, 0, 2)\n",
    "        loss = criterion(outputs_for_loss, target_labels, input_lengths, target_lengths)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = decode_predictions(outputs)\n",
    "        char_acc, seq_acc = calculate_accuracy(predictions, labels)\n",
    "        \n",
    "        # Update metrics\n",
    "        total_loss += loss.item()\n",
    "        total_char_acc += char_acc\n",
    "        total_seq_acc += seq_acc\n",
    "        batch_count += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'char_acc': f'{char_acc:.4f}',\n",
    "            'seq_acc': f'{seq_acc:.4f}'\n",
    "        })\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    avg_loss = total_loss / batch_count\n",
    "    avg_char_acc = total_char_acc / batch_count\n",
    "    avg_seq_acc = total_seq_acc / batch_count\n",
    "    \n",
    "    return avg_loss, avg_char_acc, avg_seq_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_char_acc = 0\n",
    "    total_seq_acc = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            batch_size = outputs.size(0)\n",
    "            \n",
    "            # Prepare CTC inputs\n",
    "            input_lengths = torch.full(size=(batch_size,),\n",
    "                                     fill_value=outputs.size(1),\n",
    "                                     dtype=torch.long,\n",
    "                                     device=device)\n",
    "            \n",
    "            target_lengths = []\n",
    "            target_labels = []\n",
    "            \n",
    "            for label in labels:\n",
    "                valid_label = label[label != 0]\n",
    "                target_lengths.append(len(valid_label))\n",
    "                target_labels.extend(valid_label.tolist())\n",
    "            \n",
    "            target_lengths = torch.tensor(target_lengths, dtype=torch.long, device=device)\n",
    "            target_labels = torch.tensor(target_labels, dtype=torch.long, device=device)\n",
    "            \n",
    "            # CTC loss calculation\n",
    "            outputs_for_loss = outputs.permute(1, 0, 2)\n",
    "            loss = criterion(outputs_for_loss, target_labels, input_lengths, target_lengths)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = decode_predictions(outputs)\n",
    "            char_acc, seq_acc = calculate_accuracy(predictions, labels)\n",
    "            \n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            total_char_acc += char_acc\n",
    "            total_seq_acc += seq_acc\n",
    "            batch_count += 1\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    avg_loss = total_loss / batch_count\n",
    "    avg_char_acc = total_char_acc / batch_count\n",
    "    avg_seq_acc = total_seq_acc / batch_count\n",
    "    \n",
    "    return avg_loss, avg_char_acc, avg_seq_acc\n",
    "\n",
    "# Main training loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, device='cuda'):\n",
    "    # Initialize criterion and optimizer\n",
    "    criterion = nn.CTCLoss(blank=0, reduction='mean')\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=0.01)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, \n",
    "                                 verbose=True, min_lr=1e-6)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'train_char_acc': [], 'train_seq_acc': [],\n",
    "        'val_loss': [], 'val_char_acc': [], 'val_seq_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Training device: {device}\")\n",
    "    print(f\"Initial learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss, train_char_acc, train_seq_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch\n",
    "        )\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_char_acc, val_seq_acc = validate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_char_acc'].append(train_char_acc)\n",
    "        history['train_seq_acc'].append(train_seq_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_char_acc'].append(val_char_acc)\n",
    "        history['val_seq_acc'].append(val_seq_acc)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"\\nEpoch {epoch+1} Results:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Char Acc: {train_char_acc:.4f}, Seq Acc: {train_seq_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Char Acc: {val_char_acc:.4f}, Seq Acc: {val_seq_acc:.4f}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'history': history\n",
    "            }, 'best_hybrid_model.pth')\n",
    "            print(\"New best model saved!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(\"\\nEarly stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Helper functions for decoding predictions remain the same\n",
    "import pandas as pd\n",
    "\n",
    "# Load the labels from the Excel file\n",
    "labels_df = pd.read_excel('APS360_Project_Dataset/dataset5/labels.xlsx')\n",
    "\n",
    "# Assuming your Excel has a single column with labels (1, 2, 3, ...)\n",
    "# Create a dictionary mapping from numeric labels to corresponding image filenames\n",
    "image_labels = {f'img_{int(row[0])}': int(row['card_num']) for _, row in labels_df.iterrows()}\n",
    "\n",
    "class CreditCardDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_dict, yolo_model, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Directory with all the images\n",
    "            labels_dict (dict): Dictionary mapping image names to their labels\n",
    "            yolo_model: Loaded YOLO model for credit card number detection\n",
    "            transform: Optional transform to be applied on the cropped image\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_dict = labels_dict\n",
    "        self.transform = transform\n",
    "        self.yolo_model = yolo_model\n",
    "        self.image_filenames = list(labels_dict.keys())\n",
    "        \n",
    "    def preprocess_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Load image and use YOLO to crop the credit card number region\n",
    "        \"\"\"\n",
    "        # Read image using cv2\n",
    "        original_image = cv2.imread(image_path)\n",
    "        if original_image is None:\n",
    "            raise ValueError(f\"Could not load image at {image_path}\")\n",
    "            \n",
    "        # Convert BGR to RGB\n",
    "        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get YOLO predictions\n",
    "        results = self.yolo_model(original_image)\n",
    "        \n",
    "        # Extract bounding box coordinates\n",
    "        if len(results.xyxy[0]) > 0:\n",
    "            # Get the detection with highest confidence\n",
    "            detection = results.xyxy[0][0]  # [x1, y1, x2, y2, confidence, class]\n",
    "            x1, y1, x2, y2 = map(int, detection[:4])\n",
    "            \n",
    "            # Crop the image\n",
    "            cropped_image = original_image[y1:y2, x1:x2]\n",
    "            \n",
    "            # Convert to grayscale\n",
    "            gray_image = cv2.cvtColor(cropped_image, cv2.COLOR_RGB2GRAY)\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            pil_image = Image.fromarray(gray_image)\n",
    "            \n",
    "            return pil_image\n",
    "        else:\n",
    "            # If no detection, return the original image converted to grayscale\n",
    "            print(f\"No detection found for {image_path}, using full image\")\n",
    "            gray_image = cv2.cvtColor(original_image, cv2.COLOR_RGB2GRAY)\n",
    "            pil_image = Image.fromarray(gray_image)\n",
    "            return pil_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple (image, label) where label is a list of integers\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get image filename\n",
    "            img_name = self.image_filenames[idx]\n",
    "            image_path = os.path.join(self.image_dir, img_name + '.png')\n",
    "            \n",
    "            # Process image with YOLO and get cropped region\n",
    "            image = self.preprocess_image(image_path)\n",
    "            \n",
    "            # Apply transforms if any\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            # Convert label from integer to list of integers\n",
    "            label = [int(digit) for digit in str(self.labels_dict[img_name])]\n",
    "            \n",
    "            return image, torch.tensor(label, dtype=torch.long)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_name}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "# Optional: Function to visualize the preprocessing\n",
    "def visualize_preprocessing(dataset, index):\n",
    "    \"\"\"\n",
    "    Visualize the original image, YOLO detection, and final processed image\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Get original image path\n",
    "    img_name = dataset.image_filenames[index]\n",
    "    image_path = os.path.join(dataset.image_dir, img_name + '.png')\n",
    "    \n",
    "    # Read original image\n",
    "    original_image = cv2.imread(image_path)\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get YOLO predictions\n",
    "    results = dataset.yolo_model(original_image)\n",
    "    \n",
    "    # Create a copy for drawing\n",
    "    detection_image = original_image.copy()\n",
    "    \n",
    "    # Draw detection box\n",
    "    if len(results.xyxy[0]) > 0:\n",
    "        detection = results.xyxy[0][0]\n",
    "        x1, y1, x2, y2 = map(int, detection[:4])\n",
    "        cv2.rectangle(detection_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    # Get processed image\n",
    "    processed_image, label = dataset[index]\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(detection_image)\n",
    "    axes[1].set_title('YOLO Detection')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Convert tensor to numpy for visualization\n",
    "    processed_np = processed_image.squeeze().numpy()\n",
    "    axes[2].imshow(processed_np, cmap='gray')\n",
    "    axes[2].set_title('Processed Image')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Label: {''.join(map(str, label.tolist()))}\")\n",
    "\n",
    "# Define any transformations (if needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 128)),  # Resize to your model's expected input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Split dataset into training, validation, and test sets\n",
    "train_files, test_files = train_test_split(list(image_labels.keys()), test_size=0.2, random_state=42)\n",
    "train_files, val_files = train_test_split(train_files, test_size=0.25, random_state=42)  # 60% train, 20% val\n",
    "\n",
    "# Create datasets\n",
    "train_labels_dict = {filename: image_labels[filename] for filename in train_files}\n",
    "val_labels_dict = {filename: image_labels[filename] for filename in val_files}\n",
    "test_labels_dict = {filename: image_labels[filename] for filename in test_files}\n",
    "\n",
    "train_dataset = CreditCardDataset(image_dir, train_labels_dict, yolo_model, transform=transform)\n",
    "val_dataset = CreditCardDataset(image_dir, val_labels_dict, yolo_model, transform=transform)\n",
    "test_dataset = CreditCardDataset(image_dir, test_labels_dict, yolo_model, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    \n",
    "    # Pad labels to the maximum length in the batch and convert to tensor\n",
    "    padded_labels = pad_sequence([torch.tensor(label, dtype=torch.long) for label in labels], \n",
    "                                 batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Stack images into a single tensor\n",
    "    images = torch.stack(images, dim=0)\n",
    "    \n",
    "    return images, padded_labels\n",
    "\n",
    "# Update the DataLoader to use the custom collate function\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "\n",
    "# Initialize the hybrid model\n",
    "img_height = 32  # Based on the transform resize height\n",
    "hybrid_model = HybridOCR().to(device)\n",
    "\n",
    "# Initialize CTC loss\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean')\n",
    "\n",
    "# Initialize optimizer with learning rate scheduling\n",
    "initial_lr = 0.001\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, hybrid_model.parameters()), \n",
    "                      lr=initial_lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, \n",
    "                             verbose=True, min_lr=1e-6)\n",
    "\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 128)),  # Resize to CRNN input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize grayscale image\n",
    "])\n",
    "\n",
    "# Create datasets with YOLO model\n",
    "train_dataset = CreditCardDataset(\n",
    "    image_dir=image_dir,\n",
    "    labels_dict=train_labels_dict,\n",
    "    yolo_model=yolo_model,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = CreditCardDataset(\n",
    "    image_dir=image_dir,\n",
    "    labels_dict=val_labels_dict,\n",
    "    yolo_model=yolo_model,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = CreditCardDataset(\n",
    "    image_dir=image_dir,\n",
    "    labels_dict=test_labels_dict,\n",
    "    yolo_model=yolo_model,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create DataLoaders with the custom collate function\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False, \n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False, \n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "def decode_predictions(outputs):\n",
    "    \"\"\"\n",
    "    Decode the model outputs into digit sequences\n",
    "    Args:\n",
    "        outputs: Model outputs after log_softmax (T, B, C)\n",
    "    Returns:\n",
    "        List of predicted number sequences\n",
    "    \"\"\"\n",
    "    # Convert to probabilities and get best indices\n",
    "    _, max_indices = torch.max(outputs.transpose(0, 1), 2)\n",
    "    \n",
    "    # Convert to numpy for processing\n",
    "    indices = max_indices.cpu().numpy()\n",
    "    \n",
    "    # Process each sequence in the batch\n",
    "    decoded_sequences = []\n",
    "    for sequence in indices:\n",
    "        # Remove duplicates and blanks (0)\n",
    "        current_digit = -1\n",
    "        current_sequence = []\n",
    "        \n",
    "        for digit in sequence:\n",
    "            if digit != 0 and digit != current_digit:  # Exclude blanks and duplicates\n",
    "                current_sequence.append(digit)\n",
    "                current_digit = digit\n",
    "        \n",
    "        decoded_sequences.append(current_sequence)\n",
    "    \n",
    "    return decoded_sequences\n",
    "\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    \"\"\"\n",
    "    Calculate character and sequence level accuracy\n",
    "    Args:\n",
    "        predictions: List of predicted sequences\n",
    "        targets: Tensor of target sequences (batch_size, max_length)\n",
    "    Returns:\n",
    "        (character_accuracy, sequence_accuracy)\n",
    "    \"\"\"\n",
    "    correct_chars = 0\n",
    "    total_chars = 0\n",
    "    correct_sequences = 0\n",
    "    total_sequences = 0\n",
    "    \n",
    "    for pred, target in zip(predictions, targets):\n",
    "        # Convert target tensor to list, removing padding\n",
    "        target_seq = [int(x) for x in target if x != 0]\n",
    "        \n",
    "        # Character level accuracy\n",
    "        min_len = min(len(pred), len(target_seq))\n",
    "        correct_chars += sum(1 for i in range(min_len) if pred[i] == target_seq[i])\n",
    "        total_chars += max(len(pred), len(target_seq))\n",
    "        \n",
    "        # Sequence level accuracy\n",
    "        if len(pred) == len(target_seq) and all(p == t for p, t in zip(pred, target_seq)):\n",
    "            correct_sequences += 1\n",
    "        total_sequences += 1\n",
    "    \n",
    "    char_accuracy = correct_chars / total_chars if total_chars > 0 else 0\n",
    "    seq_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0\n",
    "    \n",
    "    return char_accuracy, seq_accuracy\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 50\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training device: {device}\")\n",
    "print(f\"Initial learning rate: {initial_lr}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "\n",
    "# Start training\n",
    "history = train_model(\n",
    "    model=hybrid_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "torch.save({\n",
    "    'model_state_dict': hybrid_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'history': history,\n",
    "    'final_ensemble_weights': hybrid_model.get_ensemble_weights()\n",
    "}, 'final_hybrid_model.pth')\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final ensemble weights: {hybrid_model.get_ensemble_weights()}\")\n",
    "\n",
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot losses\n",
    "    ax1.plot(history['train_loss'], label='Train')\n",
    "    ax1.plot(history['val_loss'], label='Validation')\n",
    "    ax1.set_title('Loss Over Time')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot character accuracy\n",
    "    ax2.plot(history['train_char_acc'], label='Train')\n",
    "    ax2.plot(history['val_char_acc'], label='Validation')\n",
    "    ax2.set_title('Character Accuracy Over Time')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Plot sequence accuracy\n",
    "    ax3.plot(history['train_seq_acc'], label='Train')\n",
    "    ax3.plot(history['val_seq_acc'], label='Validation')\n",
    "    ax3.set_title('Sequence Accuracy Over Time')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Plot ensemble weights\n",
    "    weights = np.array(history['ensemble_weights'])\n",
    "    ax4.plot(weights[:, 0], label='CRNN Weight')\n",
    "    ax4.plot(weights[:, 1], label='DigitCNN Weight')\n",
    "    ax4.set_title('Ensemble Weights Over Time')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Weight')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcfa2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
