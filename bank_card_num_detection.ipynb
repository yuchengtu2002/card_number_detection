{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index    card_num\n",
      "0  0.jpg  62257583.0\n",
      "1  1.jpg  62257583.0\n",
      "2  2.jpg  62257583.0\n",
      "3  3.jpg  62257583.0\n",
      "4  4.jpg  62257583.0\n",
      "Warning: Failed to load image APS360_Project_Dataset/dataset1/bank_card_images_train/nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@17.654] global loadsave.cpp:241 findDecoder imread_('APS360_Project_Dataset/dataset1/bank_card_images_train/nan'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to load image APS360_Project_Dataset/dataset1/bank_card_images_train/nan\n",
      "Loaded 618 images from dataset1.\n",
      "First few labels: [62257583.0, 62257583.0, 62257583.0, 62257583.0, 62257583.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@17.888] global loadsave.cpp:241 findDecoder imread_('APS360_Project_Dataset/dataset1/bank_card_images_train/nan'): can't open/read file: check file path/integrity\n"
     ]
    }
   ],
   "source": [
    "# paths for the dataset folders and label files\n",
    "dataset1_path = 'APS360_Project_Dataset/dataset1/bank_card_images_train'\n",
    "train_labels_path = 'APS360_Project_Dataset/dataset1/bank_card_images_train_labels.csv'\n",
    "\n",
    "# Load the label CSV files\n",
    "dataset1_card_labels = pd.read_csv(train_labels_path)\n",
    "dataset1_card_labels = dataset1_card_labels[['index', 'card_num']]\n",
    "\n",
    "print(dataset1_card_labels.head())\n",
    "\n",
    "def load_images_from_dataset1(base_path, labels_df):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    labels_df['index'] = labels_df['index'].astype(str)\n",
    "    \n",
    "    for index, row in labels_df.iterrows():\n",
    "        img_file = os.path.join(base_path, row['index'])\n",
    "        image = cv2.imread(img_file)\n",
    "        \n",
    "        if image is not None:\n",
    "            images.append(image)\n",
    "            labels.append(row['card_num'])  # card_num is the label\n",
    "        else:\n",
    "            print(f\"Warning: Failed to load image {img_file}\")\n",
    "    \n",
    "    return images, labels \n",
    "\n",
    "dataset1_images, dataset1_labels = load_images_from_dataset1(dataset1_path, dataset1_card_labels)\n",
    "\n",
    "print(f\"Loaded {len(dataset1_images)} images from dataset1.\")\n",
    "print(f\"First few labels: {dataset1_labels[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for yolo object detection\n",
    "dataset2_images_path = 'APS360_Project_Dataset/dataset2/JPEGImages'\n",
    "dataset2_annotations_path = 'APS360_Project_Dataset/dataset2/Annotations'\n",
    "\n",
    "def parse_annotation(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    filename = root.find('filename').text\n",
    "    objects = []\n",
    "    \n",
    "    for obj in root.findall('object'):\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "        objects.append({\n",
    "            'bbox': [xmin, ymin, xmax, ymax]\n",
    "        })\n",
    "    \n",
    "    return filename, objects\n",
    "\n",
    "def load_images_for_dataset2(image_folder, annotation_folder):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    \n",
    "    for xml_file in os.listdir(annotation_folder):\n",
    "        if xml_file.endswith('.xml'):\n",
    "            xml_path = os.path.join(annotation_folder, xml_file)\n",
    "            filename, objects = parse_annotation(xml_path)\n",
    "            img_path = os.path.join(image_folder, filename)\n",
    "            image = cv2.imread(img_path)\n",
    "            \n",
    "            if image is not None:\n",
    "                images.append(image)\n",
    "                annotations.append(objects)\n",
    "            else:\n",
    "                print(f\"Warning: Failed to load image {img_path}\")\n",
    "    \n",
    "    return images, annotations\n",
    "\n",
    "dataset2_images, dataset2_annotations = load_images_for_dataset2(dataset2_images_path, dataset2_annotations_path)\n",
    "\n",
    "# Print the number of images and annotations loaded\n",
    "print(f\"Loaded {len(dataset2_images)} images and their corresponding annotations.\")\n",
    "print(f\"First few annotations: {dataset2_annotations[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for yolo object detection\n",
    "dataset3_path = 'APS360_Project_Dataset/dataset3_preprocessed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 885 images from dataset4.\n",
      "First few labels: [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "dataset4_path = 'APS360_Project_Dataset/dataset4'\n",
    "\n",
    "def load_images_for_dataset4(base_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in os.listdir(base_path):\n",
    "        folder_path = os.path.join(base_path, label)\n",
    "        \n",
    "        if os.path.isdir(folder_path):\n",
    "            for img_file in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, img_file)\n",
    "                image = cv2.imread(img_path)\n",
    "                \n",
    "                if image is not None:\n",
    "                    images.append(image) \n",
    "                    labels.append(int(label))  # Folder name is the label (0-9)\n",
    "                else:\n",
    "                    print(f\"Warning: Failed to load image {img_path}\")\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "dataset4_images, dataset4_labels = load_images_for_dataset4(dataset4_path)\n",
    "\n",
    "print(f\"Loaded {len(dataset4_images)} images from dataset4.\")\n",
    "print(f\"First few labels: {dataset4_labels[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize, convert to grey scale, normalize\n",
    "def preprocess_image(image):\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = image.astype('float32') / 255.0\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "    preprocessed_images = [preprocess_image(img) for img in images]\n",
    "    return np.array(preprocessed_images)\n",
    "preprocessed_dataset1_images = preprocess_images(dataset1_images)\n",
    "preprocessed_dataset4_images = preprocess_images(dataset4_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images_and_adjust_bboxes(images, annotations):\n",
    "    preprocessed_images = []\n",
    "    preprocessed_annotations = []\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        original_height, original_width = img.shape[:2]\n",
    "        img_preprocessed = preprocess_image(img)\n",
    "        \n",
    "        # Adjust bounding boxes for new image size\n",
    "        scale_x = 224 / original_width\n",
    "        scale_y = 224 / original_height\n",
    "        \n",
    "        adjusted_bboxes = []\n",
    "        for obj in annotations[i]:\n",
    "            bbox = obj['bbox']\n",
    "            adjusted_bbox = [\n",
    "                int(bbox[0] * scale_x),\n",
    "                int(bbox[1] * scale_y),\n",
    "                int(bbox[2] * scale_x),\n",
    "                int(bbox[3] * scale_y)\n",
    "            ]\n",
    "            adjusted_bboxes.append({'bbox': adjusted_bbox})\n",
    "        \n",
    "        preprocessed_images.append(img_preprocessed)\n",
    "        preprocessed_annotations.append(adjusted_bboxes)\n",
    "    \n",
    "    return np.array(preprocessed_images), preprocessed_annotations\n",
    "\n",
    "preprocessed_dataset2_images, preprocessed_dataset2_annotations = preprocess_images_and_adjust_bboxes(dataset2_images, dataset2_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "def augment_images(images, labels, target_count):\n",
    "    augment_transform = transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomResizedCrop(size=(224,224), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    label_counts = dict(zip(unique_labels, counts))\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_images = [img for img, lbl in zip(images, labels) if lbl == label]\n",
    "        current_count = label_counts[label]\n",
    "        required_count = target_count.get(label, current_count)\n",
    "        \n",
    "        if current_count < required_count:\n",
    "            images_to_augment = random.choices(label_images, k=required_count - current_count)\n",
    "            for img in images_to_augment:\n",
    "                img_pil = Image.fromarray(img)  \n",
    "                augmented_img = augment_transform(img_pil)\n",
    "                aug_img_np = np.array(augmented_img.permute(1, 2, 0)) \n",
    "                aug_img_np = cv2.resize(aug_img_np, (224,224))  \n",
    "                augmented_images.append(aug_img_np)\n",
    "                augmented_labels.append(label)\n",
    "    \n",
    "    return augmented_images, augmented_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 885\n",
      "New dataset size after augmentation: 1500\n"
     ]
    }
   ],
   "source": [
    "target_count = {\n",
    "    0: 150,\n",
    "    1: 150,\n",
    "    2: 150,\n",
    "    3: 150,\n",
    "    4: 150,\n",
    "    5: 150,\n",
    "    6: 150,\n",
    "    7: 150,\n",
    "    8: 150,\n",
    "    9: 150\n",
    "}\n",
    "\n",
    "augmented_images, augmented_labels = augment_images(preprocessed_dataset4_images, dataset4_labels, target_count)\n",
    "\n",
    "balanced_dataset4_images = np.concatenate((preprocessed_dataset4_images, augmented_images), axis=0)\n",
    "balanced_dataset4_labels = np.concatenate((dataset4_labels, augmented_labels), axis=0)\n",
    "\n",
    "print(f\"Original dataset size: {len(dataset4_labels)}\")\n",
    "print(f\"New dataset size after augmentation: {len(balanced_dataset4_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
