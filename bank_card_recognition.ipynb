{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "# 1. Detect card number area with YOLO\n",
    "def detect_card_number_area(yolo_model, image):\n",
    "    # Run YOLO inference\n",
    "    results = yolo_model(image)\n",
    "    bounding_boxes = results.xyxy[0]  # Extract bounding boxes\n",
    "    # Assume one bounding box containing the card number\n",
    "    x1, y1, x2, y2 = map(int, bounding_boxes[0][:4])\n",
    "    card_number_area = image[y1:y2, x1:x2]\n",
    "    return card_number_area\n",
    "\n",
    "# 2. Segment digits in the detected card number area\n",
    "def segment_digits(card_number_area):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(card_number_area, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply binary thresholding\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Sort contours from left to right based on x-coordinate\n",
    "    contours = sorted(contours, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
    "    \n",
    "    digit_images = []\n",
    "    for ctr in contours:\n",
    "        # Get bounding box for each contour\n",
    "        x, y, w, h = cv2.boundingRect(ctr)\n",
    "        \n",
    "        # Filter contours that may not be digits based on size (you can adjust these thresholds)\n",
    "        if h > 10 and w > 5:  # Threshold values to ignore noise\n",
    "            # Crop the digit from the image using the bounding box\n",
    "            digit = gray[y:y+h, x:x+w]\n",
    "            \n",
    "            # Resize the digit to the CNN input size, e.g., 224x224\n",
    "            digit = cv2.resize(digit, (224, 224))\n",
    "            \n",
    "            # Add the preprocessed digit to the list\n",
    "            digit_images.append(digit)\n",
    "    \n",
    "    return digit_images\n",
    "\n",
    "# 3. Recognize digits using CNN\n",
    "def recognize_digits(cnn_model, digit_images):\n",
    "    cnn_model.eval()\n",
    "    card_number = \"\"\n",
    "    for digit_image in digit_images:\n",
    "        # Preprocess each digit image (resize, normalize, etc.)\n",
    "        digit_image = preprocess_digit_image(digit_image)\n",
    "        # Convert to torch tensor and add batch dimension\n",
    "        digit_image = torch.tensor(digit_image).unsqueeze(0).to(device)\n",
    "        # Predict digit\n",
    "        with torch.no_grad():\n",
    "            output = cnn_model(digit_image)\n",
    "            _, predicted_digit = torch.max(output, 1)\n",
    "            card_number += str(predicted_digit.item())\n",
    "    return card_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DigitCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitCNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=2),\n",
    "            nn.BatchNorm2d(32, momentum=0.1)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=2),\n",
    "            nn.BatchNorm2d(64, momentum=0.1)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=2),\n",
    "            nn.BatchNorm2d(128, momentum=0.1)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(128 * 29 * 29, 128)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 classes for digits 0-9\n",
    "        self\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 128 * 29 * 29)\n",
    "        x = F.relu(self.dropout(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2024-11-3 Python-3.12.3 torch-2.3.1 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m raw_card_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPS360_Project_Dataset/dataset5/img/img_1.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m yolo_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m'\u001b[39m, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov5/yolov5s.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m card_number_area \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_card_number_area\u001b[49m\u001b[43m(\u001b[49m\u001b[43myolo_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_card_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m digit_images \u001b[38;5;241m=\u001b[39m segment_digits(card_number_area)\n\u001b[0;32m      5\u001b[0m cnn_model \u001b[38;5;241m=\u001b[39m DigitCNN()\n",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m, in \u001b[0;36mdetect_card_number_area\u001b[1;34m(yolo_model, image)\u001b[0m\n\u001b[0;32m      8\u001b[0m bounding_boxes \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mxyxy[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Extract bounding boxes\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Assume one bounding box containing the card number\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, \u001b[43mbounding_boxes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[:\u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m     11\u001b[0m card_number_area \u001b[38;5;241m=\u001b[39m image[y1:y2, x1:x2]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m card_number_area\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "raw_card_image = cv2.imread(\"APS360_Project_Dataset/dataset5/img/img_1.png\")\n",
    "yolo_model = torch.hub.load('yolov5', 'custom', path=\"yolov5/yolov5s.pt\", source='local')\n",
    "card_number_area = detect_card_number_area(yolo_model, raw_card_image)\n",
    "digit_images = segment_digits(card_number_area)\n",
    "cnn_model = DigitCNN()\n",
    "cnn_model.load_state_dict(torch.load(\"CNN_bs128_lr0.001_SGD_94valacc.pth\", map_location=torch.device('cpu')))\n",
    "card_number = recognize_digits(cnn_model, digit_images)\n",
    "print(\"Detected Card Number:\", card_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
