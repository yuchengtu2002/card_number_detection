{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda' \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'   \n",
    "else:\n",
    "    device = 'cpu'  \n",
    "\n",
    "# Set dataset path\n",
    "dataset_path = 'APS360_Project_Dataset/dataset4'\n",
    "\n",
    "# Augmentation transform\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Preprocessing transform (normalization)\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Data loading and preprocessing function\n",
    "def load_digits_and_preprocess(base_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in os.listdir(base_path):\n",
    "        folder_path = os.path.join(base_path, label)\n",
    "        \n",
    "        if os.path.isdir(folder_path):\n",
    "            for img_file in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, img_file)\n",
    "                img = cv2.imread(img_path)\n",
    "                \n",
    "                if img is not None:\n",
    "                    # Convert to PIL for transformation\n",
    "                    img_pil = Image.fromarray(img)\n",
    "                    # Apply augmentation\n",
    "                    augmented_img = augment_transform(img_pil)\n",
    "                    # Convert back to numpy and resize\n",
    "                    aug_img_np = np.array(augmented_img.permute(1, 2, 0))\n",
    "                    aug_img_np = cv2.resize(aug_img_np, (64, 64))  # Resize to 64x64\n",
    "                    images.append(aug_img_np)\n",
    "                    labels.append(int(label))  # Folder name is the label (0-9)\n",
    "                else:\n",
    "                    print(f\"Warning: Failed to load image {img_path}\")\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def split_data(images, labels, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=test_size, random_state=random_state)\n",
    "    train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=val_size, random_state=random_state)\n",
    "    return train_images, train_labels, val_images, val_labels, test_images, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if image.dtype != np.uint8:\n",
    "            image = (image * 255).astype(np.uint8)  # Rescale if in [0, 1] range\n",
    "        \n",
    "        if self.transform:\n",
    "            image = Image.fromarray(image)\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "def create_dataloaders(batch_size, train_images, train_labels, val_images, val_labels, test_images, test_labels):\n",
    "    # Create Dataset instances\n",
    "    train_dataset = DigitDataset(train_images, train_labels, transform=data_transform)\n",
    "    val_dataset = DigitDataset(val_images, val_labels, transform=data_transform)\n",
    "    test_dataset = DigitDataset(test_images, test_labels, transform=data_transform)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 817 images\n",
      "Validation set: 91 images\n",
      "Test set: 228 images\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "images, labels = load_digits_and_preprocess(dataset_path)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_images, train_labels, val_images, val_labels, test_images, test_labels = split_data(images, labels)\n",
    "\n",
    "# Create DataLoaders with specified batch size\n",
    "batch_size = 8\n",
    "train_loader, val_loader, test_loader = create_dataloaders(batch_size, train_images, train_labels, val_images, val_labels, test_images, test_labels)\n",
    "\n",
    "# Print dataset sizes for verification\n",
    "print(f\"Train set: {len(train_loader.dataset)} images\")\n",
    "print(f\"Validation set: {len(val_loader.dataset)} images\")\n",
    "print(f\"Test set: {len(test_loader.dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImprovedDigitCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedDigitCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional Layers with Batch Norm\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)  # 10 classes for digits 0-9\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = x.view(-1, 256 * 4 * 4)  # Adjust according to final feature map size\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10, save_dir='model_checkpoints'):\n",
    "    # Create the directory for saving model checkpoints if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * correct / total\n",
    "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        model_path = os.path.join(save_dir, f'model_epoch_{epoch+1}.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f'Model saved to {model_path}')\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 2.4605\n",
      "Validation Accuracy: 17.58%\n",
      "Model saved to model_checkpoints/model_epoch_1.pth\n",
      "Epoch 2/30, Loss: 2.0040\n",
      "Validation Accuracy: 51.65%\n",
      "Model saved to model_checkpoints/model_epoch_2.pth\n",
      "Epoch 3/30, Loss: 1.2768\n",
      "Validation Accuracy: 64.84%\n",
      "Model saved to model_checkpoints/model_epoch_3.pth\n",
      "Epoch 4/30, Loss: 0.8850\n",
      "Validation Accuracy: 71.43%\n",
      "Model saved to model_checkpoints/model_epoch_4.pth\n",
      "Epoch 5/30, Loss: 0.5774\n",
      "Validation Accuracy: 80.22%\n",
      "Model saved to model_checkpoints/model_epoch_5.pth\n",
      "Epoch 6/30, Loss: 0.4912\n",
      "Validation Accuracy: 85.71%\n",
      "Model saved to model_checkpoints/model_epoch_6.pth\n",
      "Epoch 7/30, Loss: 0.2510\n",
      "Validation Accuracy: 82.42%\n",
      "Model saved to model_checkpoints/model_epoch_7.pth\n",
      "Epoch 8/30, Loss: 0.1655\n",
      "Validation Accuracy: 85.71%\n",
      "Model saved to model_checkpoints/model_epoch_8.pth\n",
      "Epoch 9/30, Loss: 0.1171\n",
      "Validation Accuracy: 86.81%\n",
      "Model saved to model_checkpoints/model_epoch_9.pth\n",
      "Epoch 10/30, Loss: 0.0782\n",
      "Validation Accuracy: 86.81%\n",
      "Model saved to model_checkpoints/model_epoch_10.pth\n",
      "Epoch 11/30, Loss: 0.0446\n",
      "Validation Accuracy: 87.91%\n",
      "Model saved to model_checkpoints/model_epoch_11.pth\n",
      "Epoch 12/30, Loss: 0.0176\n",
      "Validation Accuracy: 92.31%\n",
      "Model saved to model_checkpoints/model_epoch_12.pth\n",
      "Epoch 13/30, Loss: 0.0100\n",
      "Validation Accuracy: 89.01%\n",
      "Model saved to model_checkpoints/model_epoch_13.pth\n",
      "Epoch 14/30, Loss: 0.0049\n",
      "Validation Accuracy: 91.21%\n",
      "Model saved to model_checkpoints/model_epoch_14.pth\n",
      "Epoch 15/30, Loss: 0.0036\n",
      "Validation Accuracy: 92.31%\n",
      "Model saved to model_checkpoints/model_epoch_15.pth\n",
      "Epoch 16/30, Loss: 0.0034\n",
      "Validation Accuracy: 91.21%\n",
      "Model saved to model_checkpoints/model_epoch_16.pth\n",
      "Epoch 17/30, Loss: 0.0022\n",
      "Validation Accuracy: 91.21%\n",
      "Model saved to model_checkpoints/model_epoch_17.pth\n",
      "Epoch 18/30, Loss: 0.0019\n",
      "Validation Accuracy: 90.11%\n",
      "Model saved to model_checkpoints/model_epoch_18.pth\n",
      "Epoch 19/30, Loss: 0.0017\n",
      "Validation Accuracy: 91.21%\n",
      "Model saved to model_checkpoints/model_epoch_19.pth\n",
      "Epoch 20/30, Loss: 0.0014\n",
      "Validation Accuracy: 91.21%\n",
      "Model saved to model_checkpoints/model_epoch_20.pth\n",
      "Epoch 21/30, Loss: 0.0012\n",
      "Validation Accuracy: 91.21%\n",
      "Model saved to model_checkpoints/model_epoch_21.pth\n",
      "Epoch 22/30, Loss: 0.0011\n",
      "Validation Accuracy: 91.21%\n",
      "Model saved to model_checkpoints/model_epoch_22.pth\n",
      "Epoch 23/30, Loss: 0.0011\n",
      "Validation Accuracy: 91.21%\n",
      "Model saved to model_checkpoints/model_epoch_23.pth\n",
      "Epoch 24/30, Loss: 0.0009\n",
      "Validation Accuracy: 91.21%\n",
      "Model saved to model_checkpoints/model_epoch_24.pth\n",
      "Epoch 25/30, Loss: 0.0008\n",
      "Validation Accuracy: 91.21%\n",
      "Model saved to model_checkpoints/model_epoch_25.pth\n",
      "Epoch 26/30, Loss: 0.0006\n",
      "Validation Accuracy: 90.11%\n",
      "Model saved to model_checkpoints/model_epoch_26.pth\n",
      "Epoch 27/30, Loss: 0.0005\n",
      "Validation Accuracy: 92.31%\n",
      "Model saved to model_checkpoints/model_epoch_27.pth\n",
      "Epoch 28/30, Loss: 0.0005\n",
      "Validation Accuracy: 90.11%\n",
      "Model saved to model_checkpoints/model_epoch_28.pth\n",
      "Epoch 29/30, Loss: 0.0005\n",
      "Validation Accuracy: 91.21%\n",
      "Model saved to model_checkpoints/model_epoch_29.pth\n",
      "Epoch 30/30, Loss: 0.0019\n",
      "Validation Accuracy: 90.11%\n",
      "Model saved to model_checkpoints/model_epoch_30.pth\n",
      "Test Accuracy: 84.65%\n"
     ]
    }
   ],
   "source": [
    "model = ImprovedDigitCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs = 30)\n",
    "\n",
    "# Test the model\n",
    "test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.65%\n"
     ]
    }
   ],
   "source": [
    "cnn_model = ImprovedDigitCNN()\n",
    "cnn_model.load_state_dict(torch.load('model_checkpoints/model_epoch_30.pth', map_location=device))\n",
    "cnn_model.to(device)\n",
    "\n",
    "test_model(cnn_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
