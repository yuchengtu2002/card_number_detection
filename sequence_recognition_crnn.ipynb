{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "print(os.environ.get(\"PYTORCH_ENABLE_MPS_FALLBACK\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
>>>>>>> 7b62d41fb429f0d45adb9cee856978cd7902e5bc
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "YOLOv5  2024-11-3 Python-3.12.3 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n"
=======
      "YOLOv5 ðŸš€ 2024-11-1 Python-3.9.13 torch-2.2.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
>>>>>>> 7b62d41fb429f0d45adb9cee856978cd7902e5bc
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Error during loading: A load persistent id instruction was encountered, but no persistent_load function was specified.\n",
      "CUDA is available!\n",
      "Using device: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Error loading weights: cannot instantiate 'PosixPath' on your system\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "cannot instantiate 'PosixPath' on your system. Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Lione\\Desktop\\APS360\\APS360 Project\\yolov5\\hubconf.py:70\u001b[0m, in \u001b[0;36m_create\u001b[1;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mDetectMultiBackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoshape\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# detection model\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m autoshape:\n",
      "File \u001b[1;32mc:\\Users\\Lione\\Desktop\\APS360\\APS360 Project\\yolov5\\models\\common.py:489\u001b[0m, in \u001b[0;36mDetectMultiBackend.__init__\u001b[1;34m(self, weights, device, dnn, data, fp16, fuse)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pt:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m     stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mint\u001b[39m(model\u001b[38;5;241m.\u001b[39mstride\u001b[38;5;241m.\u001b[39mmax()), \u001b[38;5;241m32\u001b[39m)  \u001b[38;5;66;03m# model stride\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\Desktop\\APS360\\APS360 Project\\yolov5\\models\\experimental.py:98\u001b[0m, in \u001b[0;36mattempt_load\u001b[1;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [weights]:\n\u001b[1;32m---> 98\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattempt_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\ultralytics\\utils\\patches.py:86\u001b[0m, in \u001b[0;36mtorch_load\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_torch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\torch\\serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\torch\\serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\pathlib.py:1422\u001b[0m, in \u001b[0;36mPosixPath.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 1422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: cannot instantiate 'PosixPath' on your system",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Lione\\Desktop\\APS360\\APS360 Project\\yolov5\\hubconf.py:85\u001b[0m, in \u001b[0;36m_create\u001b[1;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# arbitrary model\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Lione\\Desktop\\APS360\\APS360 Project\\yolov5\\models\\experimental.py:98\u001b[0m, in \u001b[0;36mattempt_load\u001b[1;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [weights]:\n\u001b[1;32m---> 98\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattempt_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\ultralytics\\utils\\patches.py:86\u001b[0m, in \u001b[0;36mtorch_load\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_torch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\torch\\serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\torch\\serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\pathlib.py:1422\u001b[0m, in \u001b[0;36mPosixPath.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 1422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: cannot instantiate 'PosixPath' on your system",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 61\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading weights: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m yolo_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\torch\\hub.py:647\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    638\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(\n\u001b[0;32m    639\u001b[0m         repo_or_dir,\n\u001b[0;32m    640\u001b[0m         force_reload,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    644\u001b[0m         skip_validation\u001b[38;5;241m=\u001b[39mskip_validation,\n\u001b[0;32m    645\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\torch\\hub.py:676\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[0;32m    675\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[1;32m--> 676\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\Lione\\Desktop\\APS360\\APS360 Project\\yolov5\\hubconf.py:135\u001b[0m, in \u001b[0;36mcustom\u001b[1;34m(path, autoshape, _verbose, device)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom\u001b[39m(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, autoshape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    Loads a custom or local YOLOv5 model from a given path with optional autoshaping and device specification.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\Desktop\\APS360\\APS360 Project\\yolov5\\hubconf.py:103\u001b[0m, in \u001b[0;36m_create\u001b[1;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[0;32m    101\u001b[0m help_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Cache may be out of date, try `force_reload=True` or see \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhelp_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for help.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(s) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: cannot instantiate 'PosixPath' on your system. Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help."
=======
      "1\n",
      "Starting training...\n",
      "Training device: mps\n",
      "Initial learning rate: 0.001\n",
      "Number of epochs: 300\n",
      "Epoch [1/300], Batch [0/15], Loss: 5.9878, Char Acc: 0.0784, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Train Loss: 2.5864, Train Char Acc: 0.0052, Train Seq Acc: 0.0000, Val Loss: 2.3328, Val Char Acc: 0.0000, Val Seq Acc: 0.0000\n",
      "Epoch [2/300], Batch [0/15], Loss: 2.1668, Char Acc: 0.0000, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Train Loss: 2.1601, Train Char Acc: 0.0000, Train Seq Acc: 0.0000, Val Loss: 2.3112, Val Char Acc: 0.0000, Val Seq Acc: 0.0000\n",
      "Epoch [3/300], Batch [0/15], Loss: 2.0962, Char Acc: 0.0000, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/300], Train Loss: 2.0943, Train Char Acc: 0.0000, Train Seq Acc: 0.0000, Val Loss: 2.2012, Val Char Acc: 0.0000, Val Seq Acc: 0.0000\n",
      "Epoch [4/300], Batch [0/15], Loss: 2.0634, Char Acc: 0.0000, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/300], Train Loss: 2.0156, Train Char Acc: 0.0000, Train Seq Acc: 0.0000, Val Loss: 2.1426, Val Char Acc: 0.0000, Val Seq Acc: 0.0000\n",
      "Epoch [5/300], Batch [0/15], Loss: 1.9016, Char Acc: 0.0000, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/300], Train Loss: 1.9441, Train Char Acc: 0.0181, Train Seq Acc: 0.0000, Val Loss: 2.0443, Val Char Acc: 0.0393, Val Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/300], Batch [0/15], Loss: 1.8672, Char Acc: 0.0377, Seq Acc: 0.0000\n",
      "Epoch [6/300], Train Loss: 1.8800, Train Char Acc: 0.0404, Train Seq Acc: 0.0000, Val Loss: 1.9797, Val Char Acc: 0.0393, Val Seq Acc: 0.0000\n",
      "Epoch [7/300], Batch [0/15], Loss: 1.9441, Char Acc: 0.0316, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/300], Train Loss: 1.8450, Train Char Acc: 0.0404, Train Seq Acc: 0.0000, Val Loss: 2.0169, Val Char Acc: 0.0393, Val Seq Acc: 0.0000\n",
      "Epoch [8/300], Batch [0/15], Loss: 1.9013, Char Acc: 0.0420, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/300], Train Loss: 1.8025, Train Char Acc: 0.0403, Train Seq Acc: 0.0000, Val Loss: 1.9379, Val Char Acc: 0.0393, Val Seq Acc: 0.0000\n",
      "Epoch [9/300], Batch [0/15], Loss: 1.8064, Char Acc: 0.0342, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/300], Train Loss: 1.7553, Train Char Acc: 0.0404, Train Seq Acc: 0.0000, Val Loss: 1.8772, Val Char Acc: 0.0393, Val Seq Acc: 0.0000\n",
      "Epoch [10/300], Batch [0/15], Loss: 1.7689, Char Acc: 0.0371, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Train Loss: 1.7092, Train Char Acc: 0.0406, Train Seq Acc: 0.0000, Val Loss: 1.8454, Val Char Acc: 0.0393, Val Seq Acc: 0.0000\n",
      "Epoch [11/300], Batch [0/15], Loss: 1.7774, Char Acc: 0.0496, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/300], Train Loss: 1.6879, Train Char Acc: 0.0404, Train Seq Acc: 0.0000, Val Loss: 1.8131, Val Char Acc: 0.0393, Val Seq Acc: 0.0000\n",
      "Epoch [12/300], Batch [0/15], Loss: 1.6976, Char Acc: 0.0421, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/300], Train Loss: 1.6456, Train Char Acc: 0.0410, Train Seq Acc: 0.0000, Val Loss: 1.8032, Val Char Acc: 0.0435, Val Seq Acc: 0.0000\n",
      "Epoch [13/300], Batch [0/15], Loss: 1.5829, Char Acc: 0.0456, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/300], Train Loss: 1.6317, Train Char Acc: 0.0443, Train Seq Acc: 0.0000, Val Loss: 1.7802, Val Char Acc: 0.0453, Val Seq Acc: 0.0000\n",
      "Epoch [14/300], Batch [0/15], Loss: 1.6943, Char Acc: 0.0531, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/300], Train Loss: 1.6188, Train Char Acc: 0.0537, Train Seq Acc: 0.0000, Val Loss: 1.8302, Val Char Acc: 0.0468, Val Seq Acc: 0.0000\n",
      "Epoch [15/300], Batch [0/15], Loss: 1.8237, Char Acc: 0.0446, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/300], Train Loss: 1.6070, Train Char Acc: 0.0569, Train Seq Acc: 0.0000, Val Loss: 1.7832, Val Char Acc: 0.0534, Val Seq Acc: 0.0000\n",
      "Epoch [16/300], Batch [0/15], Loss: 1.3729, Char Acc: 0.0914, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/300], Train Loss: 1.5784, Train Char Acc: 0.0705, Train Seq Acc: 0.0000, Val Loss: 1.7629, Val Char Acc: 0.0604, Val Seq Acc: 0.0000\n",
      "Epoch [17/300], Batch [0/15], Loss: 1.5932, Char Acc: 0.0524, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/300], Train Loss: 1.5528, Train Char Acc: 0.0744, Train Seq Acc: 0.0000, Val Loss: 1.7293, Val Char Acc: 0.0756, Val Seq Acc: 0.0000\n",
      "Epoch [18/300], Batch [0/15], Loss: 1.5477, Char Acc: 0.0813, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/300], Train Loss: 1.5380, Train Char Acc: 0.0858, Train Seq Acc: 0.0000, Val Loss: 1.7236, Val Char Acc: 0.0781, Val Seq Acc: 0.0000\n",
      "Epoch [19/300], Batch [0/15], Loss: 1.4984, Char Acc: 0.0896, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/300], Train Loss: 1.5155, Train Char Acc: 0.0904, Train Seq Acc: 0.0000, Val Loss: 1.7089, Val Char Acc: 0.0789, Val Seq Acc: 0.0000\n",
      "Epoch [20/300], Batch [0/15], Loss: 1.6516, Char Acc: 0.0829, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/300], Train Loss: 1.4995, Train Char Acc: 0.0941, Train Seq Acc: 0.0000, Val Loss: 1.7048, Val Char Acc: 0.0800, Val Seq Acc: 0.0000\n",
      "Epoch [21/300], Batch [0/15], Loss: 1.6293, Char Acc: 0.0903, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/300], Train Loss: 1.4810, Train Char Acc: 0.0989, Train Seq Acc: 0.0000, Val Loss: 1.6784, Val Char Acc: 0.0866, Val Seq Acc: 0.0000\n",
      "Epoch [22/300], Batch [0/15], Loss: 1.4581, Char Acc: 0.0957, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/300], Train Loss: 1.4647, Train Char Acc: 0.1052, Train Seq Acc: 0.0000, Val Loss: 1.6757, Val Char Acc: 0.0875, Val Seq Acc: 0.0000\n",
      "Epoch [23/300], Batch [0/15], Loss: 1.3650, Char Acc: 0.0891, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/300], Train Loss: 1.4454, Train Char Acc: 0.1022, Train Seq Acc: 0.0000, Val Loss: 1.6633, Val Char Acc: 0.0946, Val Seq Acc: 0.0000\n",
      "Epoch [24/300], Batch [0/15], Loss: 1.1976, Char Acc: 0.1400, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/300], Train Loss: 1.4254, Train Char Acc: 0.1070, Train Seq Acc: 0.0000, Val Loss: 1.6592, Val Char Acc: 0.0923, Val Seq Acc: 0.0000\n",
      "Epoch [25/300], Batch [0/15], Loss: 1.4495, Char Acc: 0.1321, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/300], Train Loss: 1.4095, Train Char Acc: 0.1119, Train Seq Acc: 0.0000, Val Loss: 1.6743, Val Char Acc: 0.0967, Val Seq Acc: 0.0000\n",
      "Epoch [26/300], Batch [0/15], Loss: 1.4340, Char Acc: 0.1133, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/300], Train Loss: 1.3887, Train Char Acc: 0.1106, Train Seq Acc: 0.0000, Val Loss: 1.6424, Val Char Acc: 0.0920, Val Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/300], Batch [0/15], Loss: 1.3835, Char Acc: 0.1105, Seq Acc: 0.0000\n",
      "Epoch [27/300], Train Loss: 1.3775, Train Char Acc: 0.1142, Train Seq Acc: 0.0000, Val Loss: 1.6505, Val Char Acc: 0.1014, Val Seq Acc: 0.0000\n",
      "Epoch [28/300], Batch [0/15], Loss: 1.3308, Char Acc: 0.1319, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/300], Train Loss: 1.3675, Train Char Acc: 0.1219, Train Seq Acc: 0.0000, Val Loss: 1.6532, Val Char Acc: 0.1000, Val Seq Acc: 0.0000\n",
      "Epoch [29/300], Batch [0/15], Loss: 1.3181, Char Acc: 0.0926, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/300], Train Loss: 1.3509, Train Char Acc: 0.1206, Train Seq Acc: 0.0000, Val Loss: 1.6309, Val Char Acc: 0.1017, Val Seq Acc: 0.0000\n",
      "Epoch [30/300], Batch [0/15], Loss: 1.4342, Char Acc: 0.1174, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/300], Train Loss: 1.3105, Train Char Acc: 0.1338, Train Seq Acc: 0.0000, Val Loss: 1.6026, Val Char Acc: 0.0985, Val Seq Acc: 0.0000\n",
      "Epoch [31/300], Batch [0/15], Loss: 1.2164, Char Acc: 0.1225, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/300], Train Loss: 1.2825, Train Char Acc: 0.1319, Train Seq Acc: 0.0000, Val Loss: 1.6037, Val Char Acc: 0.1110, Val Seq Acc: 0.0000\n",
      "Epoch [32/300], Batch [0/15], Loss: 1.4020, Char Acc: 0.1051, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/300], Train Loss: 1.2560, Train Char Acc: 0.1397, Train Seq Acc: 0.0000, Val Loss: 1.5945, Val Char Acc: 0.1137, Val Seq Acc: 0.0000\n",
      "Epoch [33/300], Batch [0/15], Loss: 1.2040, Char Acc: 0.1350, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/300], Train Loss: 1.2369, Train Char Acc: 0.1511, Train Seq Acc: 0.0000, Val Loss: 1.6005, Val Char Acc: 0.1361, Val Seq Acc: 0.0000\n",
      "Epoch [34/300], Batch [0/15], Loss: 1.1386, Char Acc: 0.1638, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/300], Train Loss: 1.2079, Train Char Acc: 0.1594, Train Seq Acc: 0.0000, Val Loss: 1.5888, Val Char Acc: 0.1157, Val Seq Acc: 0.0000\n",
      "Epoch [35/300], Batch [0/15], Loss: 1.1282, Char Acc: 0.1649, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/300], Train Loss: 1.1902, Train Char Acc: 0.1649, Train Seq Acc: 0.0000, Val Loss: 1.6067, Val Char Acc: 0.1110, Val Seq Acc: 0.0000\n",
      "Epoch [36/300], Batch [0/15], Loss: 1.0717, Char Acc: 0.1761, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/300], Train Loss: 1.1583, Train Char Acc: 0.1727, Train Seq Acc: 0.0000, Val Loss: 1.5741, Val Char Acc: 0.1344, Val Seq Acc: 0.0000\n",
      "Epoch [37/300], Batch [0/15], Loss: 1.1652, Char Acc: 0.1576, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/300], Train Loss: 1.1273, Train Char Acc: 0.1945, Train Seq Acc: 0.0000, Val Loss: 1.5307, Val Char Acc: 0.1664, Val Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/300], Batch [0/15], Loss: 1.2106, Char Acc: 0.2205, Seq Acc: 0.0000\n",
      "Epoch [38/300], Train Loss: 1.1005, Train Char Acc: 0.2051, Train Seq Acc: 0.0000, Val Loss: 1.5146, Val Char Acc: 0.1431, Val Seq Acc: 0.0000\n",
      "Epoch [39/300], Batch [0/15], Loss: 0.9108, Char Acc: 0.2369, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/300], Train Loss: 1.0741, Train Char Acc: 0.2178, Train Seq Acc: 0.0000, Val Loss: 1.5021, Val Char Acc: 0.1774, Val Seq Acc: 0.0000\n",
      "Epoch [40/300], Batch [0/15], Loss: 1.1130, Char Acc: 0.1880, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/300], Train Loss: 1.0469, Train Char Acc: 0.2292, Train Seq Acc: 0.0021, Val Loss: 1.5264, Val Char Acc: 0.2043, Val Seq Acc: 0.0000\n",
      "Epoch [41/300], Batch [0/15], Loss: 1.0505, Char Acc: 0.2164, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/300], Train Loss: 1.0280, Train Char Acc: 0.2445, Train Seq Acc: 0.0042, Val Loss: 1.5007, Val Char Acc: 0.2041, Val Seq Acc: 0.0000\n",
      "Epoch [42/300], Batch [0/15], Loss: 0.9282, Char Acc: 0.2702, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/300], Train Loss: 0.9993, Train Char Acc: 0.2642, Train Seq Acc: 0.0042, Val Loss: 1.5064, Val Char Acc: 0.2092, Val Seq Acc: 0.0000\n",
      "Epoch [43/300], Batch [0/15], Loss: 0.8845, Char Acc: 0.3229, Seq Acc: 0.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/300], Train Loss: 0.9604, Train Char Acc: 0.2821, Train Seq Acc: 0.0063, Val Loss: 1.5070, Val Char Acc: 0.1950, Val Seq Acc: 0.0000\n",
      "Epoch [44/300], Batch [0/15], Loss: 0.9027, Char Acc: 0.2826, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/300], Train Loss: 0.9411, Train Char Acc: 0.3013, Train Seq Acc: 0.0042, Val Loss: 1.4671, Val Char Acc: 0.2370, Val Seq Acc: 0.0000\n",
      "Epoch [45/300], Batch [0/15], Loss: 1.1182, Char Acc: 0.2459, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/300], Train Loss: 0.9084, Train Char Acc: 0.3162, Train Seq Acc: 0.0083, Val Loss: 1.4608, Val Char Acc: 0.2453, Val Seq Acc: 0.0000\n",
      "Epoch [46/300], Batch [0/15], Loss: 0.8516, Char Acc: 0.3583, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/300], Train Loss: 0.8770, Train Char Acc: 0.3351, Train Seq Acc: 0.0125, Val Loss: 1.4692, Val Char Acc: 0.2753, Val Seq Acc: 0.0000\n",
      "Epoch [47/300], Batch [0/15], Loss: 0.8430, Char Acc: 0.3630, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/300], Train Loss: 0.8451, Train Char Acc: 0.3515, Train Seq Acc: 0.0167, Val Loss: 1.4297, Val Char Acc: 0.2501, Val Seq Acc: 0.0000\n",
      "Epoch [48/300], Batch [0/15], Loss: 0.8431, Char Acc: 0.3685, Seq Acc: 0.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/300], Train Loss: 0.8128, Train Char Acc: 0.3677, Train Seq Acc: 0.0208, Val Loss: 1.4194, Val Char Acc: 0.2782, Val Seq Acc: 0.0000\n",
      "Epoch [49/300], Batch [0/15], Loss: 0.7507, Char Acc: 0.4516, Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/300], Train Loss: 0.7717, Train Char Acc: 0.3905, Train Seq Acc: 0.0229, Val Loss: 1.4557, Val Char Acc: 0.2426, Val Seq Acc: 0.0000\n",
      "Epoch [50/300], Batch [0/15], Loss: 0.6811, Char Acc: 0.4368, Seq Acc: 0.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/300], Train Loss: 0.7375, Train Char Acc: 0.4103, Train Seq Acc: 0.0312, Val Loss: 1.4214, Val Char Acc: 0.2753, Val Seq Acc: 0.0000\n",
      "Epoch [51/300], Batch [0/15], Loss: 0.6300, Char Acc: 0.4537, Seq Acc: 0.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/300], Train Loss: 0.7085, Train Char Acc: 0.4112, Train Seq Acc: 0.0312, Val Loss: 1.4373, Val Char Acc: 0.2712, Val Seq Acc: 0.0000\n",
      "Epoch [52/300], Batch [0/15], Loss: 0.6990, Char Acc: 0.3961, Seq Acc: 0.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/300], Train Loss: 0.7005, Train Char Acc: 0.4308, Train Seq Acc: 0.0312, Val Loss: 1.4407, Val Char Acc: 0.2825, Val Seq Acc: 0.0078\n",
      "Epoch [53/300], Batch [0/15], Loss: 0.7340, Char Acc: 0.4225, Seq Acc: 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/300], Train Loss: 0.6865, Train Char Acc: 0.4523, Train Seq Acc: 0.0396, Val Loss: 1.4805, Val Char Acc: 0.3031, Val Seq Acc: 0.0156\n",
      "Epoch [54/300], Batch [0/15], Loss: 0.5957, Char Acc: 0.4407, Seq Acc: 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/300], Train Loss: 0.6861, Train Char Acc: 0.4378, Train Seq Acc: 0.0271, Val Loss: 1.4893, Val Char Acc: 0.2864, Val Seq Acc: 0.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/300], Batch [0/15], Loss: 0.7294, Char Acc: 0.4453, Seq Acc: 0.0625\n",
      "Epoch [55/300], Train Loss: 0.6675, Train Char Acc: 0.4566, Train Seq Acc: 0.0479, Val Loss: 1.4555, Val Char Acc: 0.3202, Val Seq Acc: 0.0156\n",
      "Epoch [56/300], Batch [0/15], Loss: 0.6933, Char Acc: 0.4351, Seq Acc: 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/300], Train Loss: 0.6141, Train Char Acc: 0.4888, Train Seq Acc: 0.0542, Val Loss: 1.4696, Val Char Acc: 0.3537, Val Seq Acc: 0.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/300], Batch [0/15], Loss: 0.5805, Char Acc: 0.5429, Seq Acc: 0.0938\n",
      "Epoch [57/300], Train Loss: 0.5792, Train Char Acc: 0.4995, Train Seq Acc: 0.0500, Val Loss: 1.4922, Val Char Acc: 0.3296, Val Seq Acc: 0.0078\n",
      "Epoch [58/300], Batch [0/15], Loss: 0.4373, Char Acc: 0.5935, Seq Acc: 0.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/300], Train Loss: 0.5449, Train Char Acc: 0.5258, Train Seq Acc: 0.0583, Val Loss: 1.4044, Val Char Acc: 0.3549, Val Seq Acc: 0.0312\n",
      "Epoch [59/300], Batch [0/15], Loss: 0.4891, Char Acc: 0.5037, Seq Acc: 0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/300], Train Loss: 0.5042, Train Char Acc: 0.5442, Train Seq Acc: 0.0875, Val Loss: 1.4491, Val Char Acc: 0.3641, Val Seq Acc: 0.0234\n",
      "Epoch [60/300], Batch [0/15], Loss: 0.4500, Char Acc: 0.4668, Seq Acc: 0.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/300], Train Loss: 0.4837, Train Char Acc: 0.5610, Train Seq Acc: 0.0875, Val Loss: 1.3979, Val Char Acc: 0.3348, Val Seq Acc: 0.0234\n",
      "Epoch [61/300], Batch [0/15], Loss: 0.5084, Char Acc: 0.4882, Seq Acc: 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/300], Train Loss: 0.4570, Train Char Acc: 0.5665, Train Seq Acc: 0.1000, Val Loss: 1.4518, Val Char Acc: 0.3604, Val Seq Acc: 0.0156\n",
      "Epoch [62/300], Batch [0/15], Loss: 0.4058, Char Acc: 0.5556, Seq Acc: 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/300], Train Loss: 0.4350, Train Char Acc: 0.5928, Train Seq Acc: 0.1167, Val Loss: 1.4119, Val Char Acc: 0.3746, Val Seq Acc: 0.0234\n",
      "Epoch [63/300], Batch [0/15], Loss: 0.2992, Char Acc: 0.6732, Seq Acc: 0.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/300], Train Loss: 0.4085, Train Char Acc: 0.6092, Train Seq Acc: 0.1375, Val Loss: 1.4352, Val Char Acc: 0.3764, Val Seq Acc: 0.0156\n",
      "Epoch [64/300], Batch [0/15], Loss: 0.3694, Char Acc: 0.5993, Seq Acc: 0.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/300], Train Loss: 0.4097, Train Char Acc: 0.6176, Train Seq Acc: 0.1396, Val Loss: 1.4534, Val Char Acc: 0.3849, Val Seq Acc: 0.0260\n",
      "Epoch [65/300], Batch [0/15], Loss: 0.3427, Char Acc: 0.5292, Seq Acc: 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/300], Train Loss: 0.3913, Train Char Acc: 0.6281, Train Seq Acc: 0.1354, Val Loss: 1.4226, Val Char Acc: 0.3796, Val Seq Acc: 0.0339\n",
      "Epoch [66/300], Batch [0/15], Loss: 0.3710, Char Acc: 0.6923, Seq Acc: 0.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/300], Train Loss: 0.3988, Train Char Acc: 0.6219, Train Seq Acc: 0.1604, Val Loss: 1.4807, Val Char Acc: 0.3815, Val Seq Acc: 0.0339\n",
      "Epoch [67/300], Batch [0/15], Loss: 0.3266, Char Acc: 0.6617, Seq Acc: 0.1562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/300], Train Loss: 0.3667, Train Char Acc: 0.6517, Train Seq Acc: 0.1583, Val Loss: 1.4729, Val Char Acc: 0.3661, Val Seq Acc: 0.0182\n",
      "Epoch [68/300], Batch [0/15], Loss: 0.3757, Char Acc: 0.6309, Seq Acc: 0.2188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/300], Train Loss: 0.3291, Train Char Acc: 0.6774, Train Seq Acc: 0.2062, Val Loss: 1.4688, Val Char Acc: 0.3721, Val Seq Acc: 0.0339\n",
      "Epoch [69/300], Batch [0/15], Loss: 0.2846, Char Acc: 0.7042, Seq Acc: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/300], Train Loss: 0.3161, Train Char Acc: 0.6807, Train Seq Acc: 0.2188, Val Loss: 1.4621, Val Char Acc: 0.3903, Val Seq Acc: 0.0339\n",
      "Epoch [70/300], Batch [0/15], Loss: 0.2965, Char Acc: 0.7574, Seq Acc: 0.3438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/300], Train Loss: 0.3125, Train Char Acc: 0.6936, Train Seq Acc: 0.2417, Val Loss: 1.4513, Val Char Acc: 0.3845, Val Seq Acc: 0.0391\n",
      "Epoch [71/300], Batch [0/15], Loss: 0.2644, Char Acc: 0.8076, Seq Acc: 0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/300], Train Loss: 0.3094, Train Char Acc: 0.6985, Train Seq Acc: 0.2687, Val Loss: 1.4846, Val Char Acc: 0.3908, Val Seq Acc: 0.0339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/300], Batch [0/15], Loss: 0.2654, Char Acc: 0.7300, Seq Acc: 0.4062\n",
      "Epoch [72/300], Train Loss: 0.2806, Train Char Acc: 0.7310, Train Seq Acc: 0.2896, Val Loss: 1.5258, Val Char Acc: 0.3900, Val Seq Acc: 0.0417\n",
      "Epoch [73/300], Batch [0/15], Loss: 0.2986, Char Acc: 0.6852, Seq Acc: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/300], Train Loss: 0.2699, Train Char Acc: 0.7291, Train Seq Acc: 0.2854, Val Loss: 1.4792, Val Char Acc: 0.4119, Val Seq Acc: 0.0495\n",
      "Epoch [74/300], Batch [0/15], Loss: 0.2547, Char Acc: 0.7117, Seq Acc: 0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/300], Train Loss: 0.2463, Train Char Acc: 0.7446, Train Seq Acc: 0.3125, Val Loss: 1.4766, Val Char Acc: 0.4137, Val Seq Acc: 0.0417\n",
      "Epoch [75/300], Batch [0/15], Loss: 0.1867, Char Acc: 0.8394, Seq Acc: 0.4062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/300], Train Loss: 0.2269, Train Char Acc: 0.7662, Train Seq Acc: 0.3583, Val Loss: 1.5092, Val Char Acc: 0.3940, Val Seq Acc: 0.0234\n",
      "Epoch [76/300], Batch [0/15], Loss: 0.2395, Char Acc: 0.8386, Seq Acc: 0.4688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/300], Train Loss: 0.2119, Train Char Acc: 0.7945, Train Seq Acc: 0.4062, Val Loss: 1.5219, Val Char Acc: 0.4224, Val Seq Acc: 0.0469\n",
      "Epoch [77/300], Batch [0/15], Loss: 0.1755, Char Acc: 0.8195, Seq Acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/300], Train Loss: 0.2023, Train Char Acc: 0.7965, Train Seq Acc: 0.4167, Val Loss: 1.4781, Val Char Acc: 0.4265, Val Seq Acc: 0.0469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/300], Batch [0/15], Loss: 0.1597, Char Acc: 0.8363, Seq Acc: 0.5312\n",
      "Epoch [78/300], Train Loss: 0.1930, Train Char Acc: 0.8226, Train Seq Acc: 0.4625, Val Loss: 1.4932, Val Char Acc: 0.4067, Val Seq Acc: 0.0339\n",
      "Epoch [79/300], Batch [0/15], Loss: 0.1513, Char Acc: 0.8780, Seq Acc: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/300], Train Loss: 0.1795, Train Char Acc: 0.8439, Train Seq Acc: 0.5042, Val Loss: 1.5275, Val Char Acc: 0.4130, Val Seq Acc: 0.0495\n",
      "Epoch [80/300], Batch [0/15], Loss: 0.1820, Char Acc: 0.8490, Seq Acc: 0.4062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/300], Train Loss: 0.1709, Train Char Acc: 0.8308, Train Seq Acc: 0.4667, Val Loss: 1.5323, Val Char Acc: 0.4362, Val Seq Acc: 0.0469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/300], Batch [0/15], Loss: 0.1823, Char Acc: 0.8345, Seq Acc: 0.5938\n",
      "Epoch [81/300], Train Loss: 0.1647, Train Char Acc: 0.8498, Train Seq Acc: 0.5500, Val Loss: 1.5525, Val Char Acc: 0.4093, Val Seq Acc: 0.0391\n",
      "Epoch [82/300], Batch [0/15], Loss: 0.1460, Char Acc: 0.8577, Seq Acc: 0.6562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/300], Train Loss: 0.1562, Train Char Acc: 0.8557, Train Seq Acc: 0.5375, Val Loss: 1.5469, Val Char Acc: 0.4241, Val Seq Acc: 0.0391\n",
      "Epoch [83/300], Batch [0/15], Loss: 0.1288, Char Acc: 0.9006, Seq Acc: 0.5312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/300], Train Loss: 0.1477, Train Char Acc: 0.8674, Train Seq Acc: 0.5625, Val Loss: 1.5449, Val Char Acc: 0.4142, Val Seq Acc: 0.0391\n",
      "Epoch [84/300], Batch [0/15], Loss: 0.1023, Char Acc: 0.9423, Seq Acc: 0.7188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/300], Train Loss: 0.1466, Train Char Acc: 0.8689, Train Seq Acc: 0.5917, Val Loss: 1.5904, Val Char Acc: 0.4085, Val Seq Acc: 0.0469\n",
      "Epoch [85/300], Batch [0/15], Loss: 0.1219, Char Acc: 0.8869, Seq Acc: 0.6250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/99/qs07m3j50t3_p9zkz4_26jb40000gn/T/ipykernel_75562/2077017794.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m history = train_hybrid_model(\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrnn_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/99/qs07m3j50t3_p9zkz4_26jb40000gn/T/ipykernel_75562/2077017794.py\u001b[0m in \u001b[0;36mtrain_hybrid_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, scheduler)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mtrain_seq_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/99/qs07m3j50t3_p9zkz4_26jb40000gn/T/ipykernel_75562/2077017794.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;31m# Process image with YOLO and get cropped region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;31m# Apply transforms if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/99/qs07m3j50t3_p9zkz4_26jb40000gn/T/ipykernel_75562/2077017794.py\u001b[0m in \u001b[0;36mpreprocess_image\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# Get YOLO predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myolo_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;31m# Extract bounding box coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxyxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/APS360/card_number_detection/yolov5/models/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;31m# Post-process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/APS360/card_number_detection/yolov5/models/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# PyTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maugment\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvisualize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TorchScript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/APS360/card_number_detection/yolov5/models/yolo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# augmented inference, None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# single-scale inference, train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/APS360/card_number_detection/yolov5/models/yolo.py\u001b[0m in \u001b[0;36m_forward_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_one_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# save output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/APS360/card_number_detection/yolov5/models/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;34m\"\"\"Performs forward propagation using concatenated outputs from two convolutions and a Bottleneck sequence.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/APS360/card_number_detection/yolov5/models/common.py\u001b[0m in \u001b[0;36mforward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_fuse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;34m\"\"\"Applies a fused convolution and activation function to the input tensor `x`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
>>>>>>> 7b62d41fb429f0d45adb9cee856978cd7902e5bc
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pickle\n",
    "import pickle\n",
    "from pathlib import WindowsPath\n",
    "\n",
    "# class FixedUnpickler(pickle.Unpickler):\n",
    "#     def find_class(self, module, name):\n",
    "#         # Redirect PosixPath to WindowsPath\n",
    "#         if module == 'pathlib' and name == 'PosixPath':\n",
    "#             return WindowsPath\n",
    "#         return super().find_class(module, name)\n",
    "\n",
    "# def custom_load(file_path):\n",
    "#     with open(file_path, 'rb') as f:\n",
    "#         return FixedUnpickler(f).load()\n",
    "\n",
    "# Your weights file\n",
    "weights_path = 'yolov5/runs/train/card_number_detector6/weights/best.pt'\n",
    "\n",
    "# try:\n",
    "#     # Load weights using the fixed unpickler\n",
    "#     weights = custom_load(weights_path)\n",
    "#     # Save the compatible weights back\n",
    "#     torch.save(weights, 'compatible_weights.pt')\n",
    "#     print(\"Successfully re-saved weights as compatible_weights.pt\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during loading: {e}\")\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"CUDA is available!\")\n",
    "#     print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "#     device = torch.device('cuda:0')  # Explicitly use the first GPU\n",
    "# else:\n",
    "#     print(\"CUDA not available, falling back to CPU.\")\n",
    "#     device = torch.device('cpu')\n",
    "\n",
    "# try:\n",
    "#     weights = torch.load(weights_path, map_location='cpu')\n",
    "#     print(\"Weights loaded successfully.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading weights: {e}\")\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "yolo_model = torch.hub.load('yolov5', 'custom', path=weights_path, source = 'local', force_reload=True, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2024-11-3 Python-3.12.3 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n",
      "0.20.1+cu118\n",
      "CUDA is available!\n",
      "Using device: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Error loading weights: cannot instantiate 'PosixPath' on your system\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "cannot instantiate 'PosixPath' on your system. Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Lione\\Desktop\\APS360\\APS360 Project\\yolov5\\hubconf.py:70\u001b[0m, in \u001b[0;36m_create\u001b[1;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mDetectMultiBackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoshape\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# detection model\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m autoshape:\n",
      "File \u001b[1;32mc:\\Users\\Lione\\Desktop\\APS360\\APS360 Project\\yolov5\\models\\common.py:489\u001b[0m, in \u001b[0;36mDetectMultiBackend.__init__\u001b[1;34m(self, weights, device, dnn, data, fp16, fuse)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pt:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m     stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mint\u001b[39m(model\u001b[38;5;241m.\u001b[39mstride\u001b[38;5;241m.\u001b[39mmax()), \u001b[38;5;241m32\u001b[39m)  \u001b[38;5;66;03m# model stride\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\Desktop\\APS360\\APS360 Project\\yolov5\\models\\experimental.py:98\u001b[0m, in \u001b[0;36mattempt_load\u001b[1;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [weights]:\n\u001b[1;32m---> 98\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattempt_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\ultralytics\\utils\\patches.py:86\u001b[0m, in \u001b[0;36mtorch_load\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_torch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\torch\\serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\torch\\serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\pathlib.py:1422\u001b[0m, in \u001b[0;36mPosixPath.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 1422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: cannot instantiate 'PosixPath' on your system",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Lione\\Desktop\\APS360\\APS360 Project\\yolov5\\hubconf.py:85\u001b[0m, in \u001b[0;36m_create\u001b[1;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# arbitrary model\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Lione\\Desktop\\APS360\\APS360 Project\\yolov5\\models\\experimental.py:98\u001b[0m, in \u001b[0;36mattempt_load\u001b[1;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [weights]:\n\u001b[1;32m---> 98\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattempt_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\ultralytics\\utils\\patches.py:86\u001b[0m, in \u001b[0;36mtorch_load\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_torch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\torch\\serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\torch\\serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\pathlib.py:1422\u001b[0m, in \u001b[0;36mPosixPath.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 1422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: cannot instantiate 'PosixPath' on your system",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 44\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading weights: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m yolo_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCRNN\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img_height, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, rnn_hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\torch\\hub.py:647\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    638\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(\n\u001b[0;32m    639\u001b[0m         repo_or_dir,\n\u001b[0;32m    640\u001b[0m         force_reload,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    644\u001b[0m         skip_validation\u001b[38;5;241m=\u001b[39mskip_validation,\n\u001b[0;32m    645\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\Lione\\miniconda3\\envs\\APS360\\Lib\\site-packages\\torch\\hub.py:676\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[0;32m    675\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[1;32m--> 676\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\Lione\\Desktop\\APS360\\APS360 Project\\yolov5\\hubconf.py:135\u001b[0m, in \u001b[0;36mcustom\u001b[1;34m(path, autoshape, _verbose, device)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom\u001b[39m(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, autoshape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    Loads a custom or local YOLOv5 model from a given path with optional autoshaping and device specification.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lione\\Desktop\\APS360\\APS360 Project\\yolov5\\hubconf.py:103\u001b[0m, in \u001b[0;36m_create\u001b[1;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[0;32m    101\u001b[0m help_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Cache may be out of date, try `force_reload=True` or see \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhelp_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for help.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(s) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: cannot instantiate 'PosixPath' on your system. Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help."
     ]
    }
   ],
   "source": [
=======
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "yolo_model = torch.hub.load('yolov5', 'custom', path='yolov5/runs/train/card_number_detector6/weights/best.pt', source = 'local').to(device)\n",
    "print(os.environ.get(\"PYTORCH_ENABLE_MPS_FALLBACK\"))\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, img_height, num_classes=10, rnn_hidden_size=128):\n",
    "        super(CRNN, self).__init__()\n",
    "        \n",
    "        # CNN Layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Calculate feature height after 3 pooling layers\n",
    "        self.feature_height = img_height // 8\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.rnn = nn.LSTM(128 * self.feature_height, rnn_hidden_size, \n",
    "                           bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(rnn_hidden_size * 2, num_classes + 1)  # +1 for CTC blank\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # CNN Feature Extraction\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Prepare for RNN\n",
    "        x = x.permute(0, 3, 1, 2)  # (batch, width, channels, height)\n",
    "        x = x.contiguous().view(batch_size, -1, 128 * self.feature_height)\n",
    "        \n",
    "        # RNN\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.fc(x)  # Shape: (batch, seq_len, num_classes + 1)\n",
    "        \n",
    "        # Apply log softmax for CTC loss\n",
    "        return F.log_softmax(x, dim=2)\n",
    "\n",
    "# Instantiate the CRNN model\n",
    "crnn_model = CRNN(img_height=64, num_classes=10)  # Adjust `img_height` to match your input image height\n",
    "crnn_model.to(device)\n",
    "\n",
    "\n",
    "def train_hybrid_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                      num_epochs, device, scheduler=None):\n",
    "    best_val_accuracy = 0.0\n",
    "    history = {\n",
    "        'train_loss': [], 'train_char_acc': [], 'train_seq_acc': [],\n",
    "        'val_loss': [], 'val_char_acc': [], 'val_seq_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_char_accuracies = []\n",
    "        train_seq_accuracies = []\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)  # Shape: (batch, seq_len, num_classes + 1)\n",
    "            \n",
    "            # Prepare CTC loss inputs\n",
    "            batch_size = outputs.size(0)\n",
    "            input_lengths = torch.full(size=(batch_size,), \n",
    "                                    fill_value=outputs.size(1), \n",
    "                                    dtype=torch.long,\n",
    "                                    device=device)\n",
    "            \n",
    "            target_lengths = []\n",
    "            target_labels = []\n",
    "            for label in labels:\n",
    "                valid_label = label[label != -1]  # Remove padding\n",
    "                target_lengths.append(len(valid_label))\n",
    "                target_labels.extend(valid_label.tolist())\n",
    "            \n",
    "            target_lengths = torch.tensor(target_lengths, dtype=torch.long, device=device)\n",
    "            target_labels = torch.tensor(target_labels, dtype=torch.long, device=device)\n",
    "            \n",
    "            # CTC loss expects (seq_len, batch, num_classes)\n",
    "            outputs = outputs.permute(1, 0, 2)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, target_labels, input_lengths, target_lengths)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = decode_predictions(outputs)\n",
    "            char_acc, seq_acc = calculate_accuracy(predictions, labels)\n",
    "            \n",
    "            # Update metrics\n",
    "            running_loss += loss.item()\n",
    "            train_char_accuracies.append(char_acc)\n",
    "            train_seq_accuracies.append(seq_acc)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                      f'Batch [{batch_idx}/{len(train_loader)}], '\n",
    "                      f'Loss: {loss.item():.4f}, '\n",
    "                      f'Char Acc: {char_acc:.4f}, '\n",
    "                      f'Seq Acc: {seq_acc:.4f}')\n",
    "        \n",
    "        # Log training metrics for the epoch\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        epoch_train_char_acc = sum(train_char_accuracies) / len(train_char_accuracies)\n",
    "        epoch_train_seq_acc = sum(train_seq_accuracies) / len(train_seq_accuracies)\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_char_acc'].append(epoch_train_char_acc)\n",
    "        history['train_seq_acc'].append(epoch_train_seq_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_char_accuracies = []\n",
    "        val_seq_accuracies = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Prepare CTC loss inputs\n",
    "                batch_size = outputs.size(0)\n",
    "                input_lengths = torch.full(size=(batch_size,), \n",
    "                                        fill_value=outputs.size(1), \n",
    "                                        dtype=torch.long,\n",
    "                                        device=device)\n",
    "                \n",
    "                target_lengths = []\n",
    "                target_labels = []\n",
    "                for label in labels:\n",
    "                    valid_label = label[label != -1]  # Remove padding\n",
    "                    target_lengths.append(len(valid_label))\n",
    "                    target_labels.extend(valid_label.tolist())\n",
    "                \n",
    "                target_lengths = torch.tensor(target_lengths, dtype=torch.long, device=device)\n",
    "                target_labels = torch.tensor(target_labels, dtype=torch.long, device=device)\n",
    "                \n",
    "                # CTC loss expects (seq_len, batch, num_classes)\n",
    "                outputs = outputs.permute(1, 0, 2)\n",
    "                \n",
    "                # Calculate loss\n",
    "                val_loss = criterion(outputs, target_labels, input_lengths, target_lengths)\n",
    "                val_running_loss += val_loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                predictions = decode_predictions(outputs)\n",
    "                val_char_acc, val_seq_acc = calculate_accuracy(predictions, labels)\n",
    "                val_char_accuracies.append(val_char_acc)\n",
    "                val_seq_accuracies.append(val_seq_acc)\n",
    "        \n",
    "        # Log validation metrics for the epoch\n",
    "        epoch_val_loss = val_running_loss / len(val_loader)\n",
    "        epoch_val_char_acc = sum(val_char_accuracies) / len(val_char_accuracies)\n",
    "        epoch_val_seq_acc = sum(val_seq_accuracies) / len(val_seq_accuracies)\n",
    "        \n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_char_acc'].append(epoch_val_char_acc)\n",
    "        history['val_seq_acc'].append(epoch_val_seq_acc)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {epoch_train_loss:.4f}, Train Char Acc: {epoch_train_char_acc:.4f}, Train Seq Acc: {epoch_train_seq_acc:.4f}, '\n",
    "              f'Val Loss: {epoch_val_loss:.4f}, Val Char Acc: {epoch_val_char_acc:.4f}, Val Seq Acc: {epoch_val_seq_acc:.4f}')\n",
    "        \n",
    "        # Checkpoint the best model\n",
    "        if epoch_val_seq_acc > best_val_accuracy:\n",
    "            best_val_accuracy = epoch_val_seq_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "        \n",
    "        # Step the scheduler if provided\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
>>>>>>> 7b62d41fb429f0d45adb9cee856978cd7902e5bc
    "# Load the labels from the Excel file\n",
    "labels_df = pd.read_excel('APS360_Project_Dataset/dataset5/labels.xlsx')\n",
    "\n",
    "# Assuming your Excel has a single column with labels (1, 2, 3, ...)\n",
    "# Create a dictionary mapping from numeric labels to corresponding image filenames\n",
    "image_labels = {f'{int(row[0])}': row['card_num'] for _, row in labels_df.iterrows()}\n",
    "\n",
    "class CreditCardDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_dict, yolo_model, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Directory with all the images\n",
    "            labels_dict (dict): Dictionary mapping image names to their labels\n",
    "            yolo_model: Loaded YOLO model for credit card number detection\n",
    "            transform: Optional transform to be applied on the cropped image\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_dict = labels_dict\n",
    "        self.transform = transform\n",
    "        self.yolo_model = yolo_model\n",
    "        self.image_filenames = list(labels_dict.keys())\n",
    "        \n",
    "    def preprocess_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Load image and use YOLO to crop the credit card number region\n",
    "        \"\"\"\n",
    "        # Read image using cv2\n",
    "        original_image = cv2.imread(image_path)\n",
    "        if original_image is None:\n",
    "            raise ValueError(f\"Could not load image at {image_path}\")\n",
    "        \n",
    "        # Get YOLO predictions\n",
    "        results = self.yolo_model(original_image)\n",
    "        # Extract bounding box coordinates\n",
    "        if len(results.xyxy[0]) > 0:\n",
    "            # Get the detection with highest confidence\n",
    "            detection = results.xyxy[0][0]  # [x1, y1, x2, y2, confidence, class]\n",
    "            x1, y1, x2, y2 = map(int, detection[:4])\n",
    "            \n",
    "            \n",
    "            # Crop the image\n",
    "            cropped_image = original_image[y1:y2, x1:x2]\n",
    "            if cropped_image is None or cropped_image.size == 0:\n",
    "                gray_image = cv2.cvtColor(original_image, cv2.COLOR_RGB2GRAY)\n",
    "                pil_image = Image.fromarray(gray_image)\n",
    "                return pil_image\n",
    "            \n",
    "            \n",
    "            # Convert to grayscale\n",
    "            gray_image = cv2.cvtColor(cropped_image, cv2.COLOR_RGB2GRAY)\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            pil_image = Image.fromarray(gray_image)\n",
    "            \n",
    "            return pil_image\n",
    "        else:\n",
    "            # If no detection, return the original image converted to grayscale\n",
    "            gray_image = cv2.cvtColor(original_image, cv2.COLOR_RGB2GRAY)\n",
    "            pil_image = Image.fromarray(gray_image)\n",
    "            return pil_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple (image, label) where label is a list of integers\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get image filename\n",
    "            img_name = self.image_filenames[idx]\n",
    "            image_path = os.path.join(self.image_dir, img_name + '.jpg')\n",
    "            \n",
    "            # Process image with YOLO and get cropped region\n",
    "            image = self.preprocess_image(image_path)\n",
    "            \n",
    "            # Apply transforms if any\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            # Convert label from integer to list of integers\n",
    "            label = [int(digit) for digit in str(self.labels_dict[img_name])]\n",
    "            \n",
    "            return image, torch.tensor(label, dtype=torch.long)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_name}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "# Directory where your images are located\n",
    "image_dir = 'APS360_Project_Dataset/dataset5/img'\n",
    "\n",
    "# Split dataset into training, validation, and test sets\n",
    "train_files, val_files = train_test_split(list(image_labels.keys()), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_labels_dict = {filename: image_labels[filename] for filename in train_files}\n",
    "val_labels_dict = {filename: image_labels[filename] for filename in val_files}\n",
    "\n",
    "# Define any transformations (if needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 512)),  # Resize to your model's expected input size\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Create datasets with YOLO model\n",
    "train_dataset = CreditCardDataset(\n",
    "    image_dir=image_dir,\n",
    "    labels_dict=train_labels_dict,\n",
    "    yolo_model=yolo_model,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = CreditCardDataset(\n",
    "    image_dir=image_dir,\n",
    "    labels_dict=val_labels_dict,\n",
    "    yolo_model=yolo_model,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    \n",
    "    # Pad labels to the maximum length in the batch and convert to tensor\n",
    "    padded_labels = pad_sequence([torch.tensor(label, dtype=torch.long) for label in labels], \n",
    "                                 batch_first=True, padding_value=-1)\n",
    "    \n",
    "    # Stack images into a single tensor\n",
    "    images = torch.stack(images, dim=0)\n",
    "    \n",
    "    return images, padded_labels\n",
    "\n",
<<<<<<< HEAD
=======
    "# Update the DataLoader to use the custom collate function\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "\n",
    "# Initialize CTC loss\n",
    "criterion = nn.CTCLoss(blank=10, reduction='mean')\n",
    "\n",
    "# Initialize optimizer with learning rate scheduling\n",
    "initial_lr = 0.001\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, crnn_model.parameters()), \n",
    "                      lr=initial_lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, \n",
    "                             verbose=True, min_lr=0.001)\n",
    "\n",
>>>>>>> 7b62d41fb429f0d45adb9cee856978cd7902e5bc
    "def decode_predictions(outputs):\n",
    "    \"\"\"\n",
    "    Decode the model outputs into digit sequences\n",
    "    Args:\n",
    "        outputs: Model outputs after log_softmax (T, B, C)\n",
    "    Returns:\n",
    "        List of predicted number sequences\n",
    "    \"\"\"\n",
    "    # Convert to probabilities and get best indices\n",
    "    _, max_indices = torch.max(outputs.transpose(0, 1), 2)\n",
    "    \n",
    "    # Convert to numpy for processing\n",
    "    indices = max_indices.cpu().numpy()\n",
    "    \n",
    "    # Process each sequence in the batch\n",
    "    decoded_sequences = []\n",
    "    for sequence in indices:\n",
    "        # Remove duplicates and blanks (0)\n",
    "        current_digit = -1\n",
    "        current_sequence = []\n",
    "        \n",
    "        for digit in sequence:\n",
    "            if digit != 10 and digit != current_digit:  # Exclude blanks and duplicates\n",
    "                current_sequence.append(digit)\n",
    "            current_digit = digit\n",
    "        \n",
    "        decoded_sequences.append(current_sequence)\n",
    "    \n",
    "    return decoded_sequences\n",
    "\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    \"\"\"\n",
    "    Calculate character and sequence level accuracy\n",
    "    Args:\n",
    "        predictions: List of predicted sequences\n",
    "        targets: Tensor of target sequences (batch_size, max_length)\n",
    "    Returns:\n",
    "        (character_accuracy, sequence_accuracy)\n",
    "    \"\"\"\n",
    "    correct_chars = 0\n",
    "    total_chars = 0\n",
    "    correct_sequences = 0\n",
    "    total_sequences = 0\n",
    "    \n",
    "    for pred, target in zip(predictions, targets):\n",
    "        # Convert target tensor to list, removing padding\n",
    "        target_seq = [int(x) for x in target if x != -1]\n",
    "        \n",
    "        # Character level accuracy\n",
    "        min_len = min(len(pred), len(target_seq))\n",
    "        correct_chars += sum(1 for i in range(min_len) if pred[i] == target_seq[i])\n",
    "        total_chars += max(len(pred), len(target_seq))\n",
    "        \n",
    "        # Sequence level accuracy\n",
    "        if len(pred) == len(target_seq) and all(p == t for p, t in zip(pred, target_seq)):\n",
    "            correct_sequences += 1\n",
    "        total_sequences += 1\n",
    "    \n",
    "    char_accuracy = correct_chars / total_chars if total_chars > 0 else 0\n",
    "    seq_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0\n",
    "    \n",
    "    return char_accuracy, seq_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, img_height, num_classes=10, rnn_hidden_size=128):\n",
    "        super(CRNN, self).__init__()\n",
    "        \n",
    "        # CNN Layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Calculate feature height after 3 pooling layers\n",
    "        self.feature_height = img_height // 8\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.rnn = nn.LSTM(128 * self.feature_height, rnn_hidden_size, \n",
    "                           bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(rnn_hidden_size * 2, num_classes + 1)  # +1 for CTC blank\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # CNN Feature Extraction\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Prepare for RNN\n",
    "        x = x.permute(0, 3, 1, 2)  # (batch, width, channels, height)\n",
    "        x = x.contiguous().view(batch_size, -1, 128 * self.feature_height)\n",
    "        \n",
    "        # RNN\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.fc(x)  # Shape: (batch, seq_len, num_classes + 1)\n",
    "        \n",
    "        # Apply log softmax for CTC loss\n",
    "        return F.log_softmax(x, dim=2)\n",
    "\n",
    "# Instantiate the CRNN model\n",
    "crnn_model = CRNN(img_height=64, num_classes=10)  # Adjust `img_height` to match your input image height\n",
    "crnn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                      num_epochs, device, scheduler=None):\n",
    "    best_val_accuracy = 0.0\n",
    "    history = {\n",
    "        'train_loss': [], 'train_char_acc': [], 'train_seq_acc': [],\n",
    "        'val_loss': [], 'val_char_acc': [], 'val_seq_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_char_accuracies = []\n",
    "        train_seq_accuracies = []\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)  # Shape: (batch, seq_len, num_classes + 1)\n",
    "            \n",
    "            # Prepare CTC loss inputs\n",
    "            batch_size = outputs.size(0)\n",
    "            input_lengths = torch.full(size=(batch_size,), \n",
    "                                    fill_value=outputs.size(1), \n",
    "                                    dtype=torch.long,\n",
    "                                    device=device)\n",
    "            \n",
    "            target_lengths = []\n",
    "            target_labels = []\n",
    "            for label in labels:\n",
    "                valid_label = label[label != -1]  # Remove padding\n",
    "                target_lengths.append(len(valid_label))\n",
    "                target_labels.extend(valid_label.tolist())\n",
    "            \n",
    "            target_lengths = torch.tensor(target_lengths, dtype=torch.long, device=device)\n",
    "            target_labels = torch.tensor(target_labels, dtype=torch.long, device=device)\n",
    "            \n",
    "            # CTC loss expects (seq_len, batch, num_classes)\n",
    "            outputs = outputs.permute(1, 0, 2)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, target_labels, input_lengths, target_lengths)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = decode_predictions(outputs)\n",
    "            char_acc, seq_acc = calculate_accuracy(predictions, labels)\n",
    "            \n",
    "            # Update metrics\n",
    "            running_loss += loss.item()\n",
    "            train_char_accuracies.append(char_acc)\n",
    "            train_seq_accuracies.append(seq_acc)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                      f'Batch [{batch_idx}/{len(train_loader)}], '\n",
    "                      f'Loss: {loss.item():.4f}, '\n",
    "                      f'Char Acc: {char_acc:.4f}, '\n",
    "                      f'Seq Acc: {seq_acc:.4f}')\n",
    "        \n",
    "        # Log training metrics for the epoch\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        epoch_train_char_acc = sum(train_char_accuracies) / len(train_char_accuracies)\n",
    "        epoch_train_seq_acc = sum(train_seq_accuracies) / len(train_seq_accuracies)\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_char_acc'].append(epoch_train_char_acc)\n",
    "        history['train_seq_acc'].append(epoch_train_seq_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_char_accuracies = []\n",
    "        val_seq_accuracies = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Prepare CTC loss inputs\n",
    "                batch_size = outputs.size(0)\n",
    "                input_lengths = torch.full(size=(batch_size,), \n",
    "                                        fill_value=outputs.size(1), \n",
    "                                        dtype=torch.long,\n",
    "                                        device=device)\n",
    "                \n",
    "                target_lengths = []\n",
    "                target_labels = []\n",
    "                for label in labels:\n",
    "                    valid_label = label[label != -1]  # Remove padding\n",
    "                    target_lengths.append(len(valid_label))\n",
    "                    target_labels.extend(valid_label.tolist())\n",
    "                \n",
    "                target_lengths = torch.tensor(target_lengths, dtype=torch.long, device=device)\n",
    "                target_labels = torch.tensor(target_labels, dtype=torch.long, device=device)\n",
    "                \n",
    "                # CTC loss expects (seq_len, batch, num_classes)\n",
    "                outputs = outputs.permute(1, 0, 2)\n",
    "                \n",
    "                # Calculate loss\n",
    "                val_loss = criterion(outputs, target_labels, input_lengths, target_lengths)\n",
    "                val_running_loss += val_loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                predictions = decode_predictions(outputs)\n",
    "                val_char_acc, val_seq_acc = calculate_accuracy(predictions, labels)\n",
    "                val_char_accuracies.append(val_char_acc)\n",
    "                val_seq_accuracies.append(val_seq_acc)\n",
    "        \n",
    "        # Log validation metrics for the epoch\n",
    "        epoch_val_loss = val_running_loss / len(val_loader)\n",
    "        epoch_val_char_acc = sum(val_char_accuracies) / len(val_char_accuracies)\n",
    "        epoch_val_seq_acc = sum(val_seq_accuracies) / len(val_seq_accuracies)\n",
    "        \n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_char_acc'].append(epoch_val_char_acc)\n",
    "        history['val_seq_acc'].append(epoch_val_seq_acc)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {epoch_train_loss:.4f}, Train Char Acc: {epoch_train_char_acc:.4f}, Train Seq Acc: {epoch_train_seq_acc:.4f}, '\n",
    "              f'Val Loss: {epoch_val_loss:.4f}, Val Char Acc: {epoch_val_char_acc:.4f}, Val Seq Acc: {epoch_val_seq_acc:.4f}')\n",
    "        \n",
    "        # Checkpoint the best model\n",
    "        if epoch_val_seq_acc > best_val_accuracy:\n",
    "            best_val_accuracy = epoch_val_seq_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "        \n",
    "        # Step the scheduler if provided\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the DataLoader to use the custom collate function\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn =custom_collate_fn)\n",
    "\n",
    "\n",
    "# # Initialize the hybrid model\n",
    "# img_height = 64  # Based on the transform resize height\n",
    "# hybrid_model = HybridOCR(digit_cnn=digit_cnn, img_height=img_height).to(device)\n",
    "\n",
    "# Initialize CTC loss\n",
    "criterion = nn.CTCLoss(blank=10, reduction='mean')\n",
    "\n",
    "# Initialize optimizer with learning rate scheduling\n",
    "initial_lr = 0.001\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, crnn_model.parameters()), \n",
    "                      lr=initial_lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, \n",
    "                             verbose=True, min_lr=0.001)\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 300\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training device: {device}\")\n",
    "print(f\"Initial learning rate: {initial_lr}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "\n",
    "# Start training\n",
    "history = train_hybrid_model(\n",
    "    model=crnn_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    scheduler=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_images = len(train_dataset)\n",
    "num_val_images = len(val_dataset)\n",
    "\n",
    "print(f\"Number of training images: {num_train_images}\")\n",
    "print(f\"Number of validation images: {num_val_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
>>>>>>> 7b62d41fb429f0d45adb9cee856978cd7902e5bc
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Function to display images in a grid\n",
    "def show_images(images, labels, n_cols=4):\n",
    "    n_images = len(images)\n",
    "    n_rows = (n_images // n_cols) + (n_images % n_cols > 0)\n",
    "    \n",
    "    plt.figure(figsize=(3, n_rows * 3))\n",
    "    \n",
    "    for idx in range(n_images):\n",
    "        plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        plt.imshow(images[idx].permute(1, 2, 0), cmap='gray')  # Convert to (H, W, C) format\n",
    "        plt.title(f'Label: {labels[idx]}')\n",
    "        plt.axis('off')  # Hide axes\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Iterate through the train_loader to collect all images\n",
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "for inputs, targets in train_loader:\n",
    "    all_images.extend(inputs)  # Extend the list with the batch of images\n",
    "    all_labels.extend(targets.numpy())  # Extend the list with the batch of labels\n",
    "\n",
    "# Show all images in the training set\n",
    "show_images(all_images, all_labels, n_cols=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_preprocessing(test_dataset,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_accuracy(history):\n",
    "    epochs = range(1, len(history['train_char_acc']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Plot character accuracy\n",
    "    plt.plot(epochs, history['train_char_acc'], label='Train Character Accuracy')\n",
    "    plt.plot(epochs, history['val_char_acc'], label='Validation Character Accuracy')\n",
    "    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Character Accuracy')\n",
    "    plt.title('Training and Validation Character Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot\n",
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
